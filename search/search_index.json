{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to rit-k8s-rdma docs \u00b6 intro paragraph about what this project does. Pluses and minuses of it...... Introduction \u00b6 Please see the introduction page for further details. Components \u00b6 Please see the components page for further details. Install \u00b6 Please see the install page for further details. Helpful Commands and Tools \u00b6 Please see the helpful commands and tools page for further details. Glossary \u00b6 Please see the glossary page for further details.","title":"Home"},{"location":"#welcome_to_rit-k8s-rdma_docs","text":"intro paragraph about what this project does. Pluses and minuses of it......","title":"Welcome to rit-k8s-rdma docs"},{"location":"#introduction","text":"Please see the introduction page for further details.","title":"Introduction"},{"location":"#components","text":"Please see the components page for further details.","title":"Components"},{"location":"#install","text":"Please see the install page for further details.","title":"Install"},{"location":"#helpful_commands_and_tools","text":"Please see the helpful commands and tools page for further details.","title":"Helpful Commands and Tools"},{"location":"#glossary","text":"Please see the glossary page for further details.","title":"Glossary"},{"location":"components/","text":"Components \u00b6 The following are a detailed description of each of the components that make up the solution. RDMA Hardware Daemon Set \u00b6 The RDMA Harware Daemon Set is a Kubernetes Daemon Set that has two tasks. The first task is to initialize the RDMA SRIOV enabled interfaces on a node and the second task is to create a RESTful endpoint for providing metadata about the PF's and their associated VF's that exist on a given node that are part of the Kubernetes cluster. Further detail about each of the containers within the RDMA Hardware Daemon set can be seen below: Init Container - the init container is a privileged container that runs as a Kubernetes Init Container , in simplified terms this means it is run before any other container within the pod. The reason that the container needs to be run as a privileged container is because it will be working on a nodes network devices, which means it needs special access in order to configure them. The init container does two things, the first is to scan all the available interfaces on the node and determine if it is an RDMA device and whether SRIOV is enabled. For all interfaces that meet those two requirements, the init container configures each interfaces VF's to be available and ready to run. Server Container - the server container is an unprivileged container that scans through all available interfaces when starting up. It then makes a list of all interfaces that are SRIOV enabled and upon a RESTful get request from a user will return associated metadata about the container. The server container only scans the interfaces at startup because SRIOV devices can only change configurations upon a machine restart, which would rerun the init container and then the server container. The RESTful endpoint serves data in a JSON formatted list of PF's and each PF has an internal list of VF's, more info can be found here . The RESTful endpoint that the server container sets up is bound to the host network. Both the init container and the server container run under the same RDMA Hardware Daemon Set pod. When configuring how to install this pod please look at daemon set install instructions for more detail. Scheduler Extender \u00b6 CNI \u00b6 Dummy Device Plugin \u00b6 The Dummy Device Plugin is a stop gap measure for the current system. The directory /dev/infiniband is needed within any pod that requires RDMA. In order to access devices in this directory a container either needs to be run in privileged mode or a Device Plugin can return a directory that a container will have privileged access to. For obvious reasons we opted to create a Device Plugin for the sole purpose that it will give any container that requests an RDMA device the proper access to /dev/infiniband . This Dummy Device Plugin does not act like any other Device Plugin where it is meant to manage a specialized resources; that is done by the RDMA Hardware Daemon Set, Scheduler Extender, and CNI components. When configuring how to install this pod please look at dummy device plugin installation for more detail.","title":"Components"},{"location":"components/#components","text":"The following are a detailed description of each of the components that make up the solution.","title":"Components"},{"location":"components/#rdma_hardware_daemon_set","text":"The RDMA Harware Daemon Set is a Kubernetes Daemon Set that has two tasks. The first task is to initialize the RDMA SRIOV enabled interfaces on a node and the second task is to create a RESTful endpoint for providing metadata about the PF's and their associated VF's that exist on a given node that are part of the Kubernetes cluster. Further detail about each of the containers within the RDMA Hardware Daemon set can be seen below: Init Container - the init container is a privileged container that runs as a Kubernetes Init Container , in simplified terms this means it is run before any other container within the pod. The reason that the container needs to be run as a privileged container is because it will be working on a nodes network devices, which means it needs special access in order to configure them. The init container does two things, the first is to scan all the available interfaces on the node and determine if it is an RDMA device and whether SRIOV is enabled. For all interfaces that meet those two requirements, the init container configures each interfaces VF's to be available and ready to run. Server Container - the server container is an unprivileged container that scans through all available interfaces when starting up. It then makes a list of all interfaces that are SRIOV enabled and upon a RESTful get request from a user will return associated metadata about the container. The server container only scans the interfaces at startup because SRIOV devices can only change configurations upon a machine restart, which would rerun the init container and then the server container. The RESTful endpoint serves data in a JSON formatted list of PF's and each PF has an internal list of VF's, more info can be found here . The RESTful endpoint that the server container sets up is bound to the host network. Both the init container and the server container run under the same RDMA Hardware Daemon Set pod. When configuring how to install this pod please look at daemon set install instructions for more detail.","title":"RDMA Hardware Daemon Set"},{"location":"components/#scheduler_extender","text":"","title":"Scheduler Extender"},{"location":"components/#cni","text":"","title":"CNI"},{"location":"components/#dummy_device_plugin","text":"The Dummy Device Plugin is a stop gap measure for the current system. The directory /dev/infiniband is needed within any pod that requires RDMA. In order to access devices in this directory a container either needs to be run in privileged mode or a Device Plugin can return a directory that a container will have privileged access to. For obvious reasons we opted to create a Device Plugin for the sole purpose that it will give any container that requests an RDMA device the proper access to /dev/infiniband . This Dummy Device Plugin does not act like any other Device Plugin where it is meant to manage a specialized resources; that is done by the RDMA Hardware Daemon Set, Scheduler Extender, and CNI components. When configuring how to install this pod please look at dummy device plugin installation for more detail.","title":"Dummy Device Plugin"},{"location":"glossary/","text":"Glossary \u00b6 Glossary stuff...","title":"Glossary"},{"location":"glossary/#glossary","text":"Glossary stuff...","title":"Glossary"},{"location":"helpAndTools/","text":"Helpful Commands and Tools \u00b6","title":"Helpful Commands and Tools"},{"location":"helpAndTools/#helpful_commands_and_tools","text":"","title":"Helpful Commands and Tools"},{"location":"install/","text":"Install \u00b6 install stuff...","title":"Install"},{"location":"install/#install","text":"install stuff...","title":"Install"},{"location":"introduction/","text":"Introduction \u00b6 The main focus of this project was to enable bandwidth limiting and allow for multiple interfaces to be specified for RDMA interfaces on a per pod basis. The current Mellanox Solution has a number of pitfalls such as an inaccurate state between the CNI and the Device Plugin. The Device Plugin is managed by Kubernetes, so Kubernets decides which interface to allocate to a given container. This is not reflected accurately in the CNI which may give it a different interface. In the current solution a pod may have 3 containers that each request a single RDMA interface; in the solution the Device Plugin removes 3 RDMA interfaces from the available pool of interfaces, but the CNI only allocates a single interface; the state of the CNI and the Device Plugin are incorrect. One of the largest problems is that Mellanox treats RDMA interfaces as a container specified resource, when in reality network namespaces are shared across pods, so any container in a pod has access to all of the same interfaces. This becomes a problem because when specifying Device Resources within a pod yaml, they are on a per container basis. For all of the reasons above the Device Plugin approach was abandoned because it could not accommodate our goals and instead we opted for a new architecture. Architecture \u00b6 The main system architecture for our design can be seen below; the green color specifies our components for our design and the yellow color specify Kubernetes components: The main workflow for how a pod would deployed in our system begins with a request for deploying a pod going to the master nodes Kubernetes Control Process. After that request is seen the Kubernetes Scheduler creates a list of possible nodes that the pod can be placed on based on requirements of the pods yaml. This list then gets sent to our Scheduler Extender , which is in charge of deciding which nodes can support the RDMA requirements specified in the pods yaml. The Scheduler Extender contacts each nodes RDMA Hardware Daemon Set , which returns a JSON formatted list of information about a nodes RDMA VF's back to the Scheduler Extender . The Scheduler Extender than processes all of the information to find a valid node that can meet the minimum bandwidth requirements of each of the requested interfaces that is specified in the pods yaml; the list of nodes whether blank or empty is sent back to the Kubernetes Core Scheduler. If no node is valid after the scheduling calls an error is raised and the pod is not placed, this error can be seen with a Kubernetes describe command of why the pod was not placed. Assuming that the pod was able to placed on at least one valid node, the Kubelet process on the valid node gets called to setup the pod. During the pods setup process the (CNI)[components.md#cni] is called to setup the network of the pod. The (CNI)[components.md#cni] first contacts the RDMA Hardware Daemon Set on the node that is running to get an up to date list of the state of the node. It then runs the same algorithm that the Scheduler Extender had run to find the correct placements of interfaces to meet the requirements of the bandwidth limitations. The (CNI)[components.md#cni] is atomic operation, so it either completes the setup of the pod or fails and rollbacks all changes made to any interfaces. Once the (CNI)[components.md#cni] finishes, the response is sent back to the Kubelet process of the node. Limitations \u00b6 There are a couple limitations when it comes to our solution: - Mellanox Vendor - the following has only been test on a Mellanox Card - Data Plane Development Kit (DPDK) - the Mellanox solution may work with DPDK, it has not been tested with our solution (changes to (CNI)[https://github.com/rit-k8s-rdma/rit-k8s-rdma-sriov-cni] required) - Shared RDMA Device - the Mellanox solution may work with a shared RDMA interface, it has not been tested with our solution (changes to (CNI)[https://github.com/rit-k8s-rdma/rit-k8s-rdma-sriov-cni] required) - Dummy Device Plugin - the current solution requires the use of Device Plugin to give access to /dev/infiniband for open issues in Kubernets that can be found here and here . The main problem is Kubernetes does not have the ideas of a device directory that Docker has --device . Future Work \u00b6 More Vendors - making the solution more interface driven so it can be adapted to more vendors then just Mellanox. Migrating CNI - the CNI is currently at an older version and we had to bootstrap the newer one, it should be upgraded. Scheduling - opening up the scheduler to be more adaptable to customizable scheduling algorithms.","title":"Introduction"},{"location":"introduction/#introduction","text":"The main focus of this project was to enable bandwidth limiting and allow for multiple interfaces to be specified for RDMA interfaces on a per pod basis. The current Mellanox Solution has a number of pitfalls such as an inaccurate state between the CNI and the Device Plugin. The Device Plugin is managed by Kubernetes, so Kubernets decides which interface to allocate to a given container. This is not reflected accurately in the CNI which may give it a different interface. In the current solution a pod may have 3 containers that each request a single RDMA interface; in the solution the Device Plugin removes 3 RDMA interfaces from the available pool of interfaces, but the CNI only allocates a single interface; the state of the CNI and the Device Plugin are incorrect. One of the largest problems is that Mellanox treats RDMA interfaces as a container specified resource, when in reality network namespaces are shared across pods, so any container in a pod has access to all of the same interfaces. This becomes a problem because when specifying Device Resources within a pod yaml, they are on a per container basis. For all of the reasons above the Device Plugin approach was abandoned because it could not accommodate our goals and instead we opted for a new architecture.","title":"Introduction"},{"location":"introduction/#architecture","text":"The main system architecture for our design can be seen below; the green color specifies our components for our design and the yellow color specify Kubernetes components: The main workflow for how a pod would deployed in our system begins with a request for deploying a pod going to the master nodes Kubernetes Control Process. After that request is seen the Kubernetes Scheduler creates a list of possible nodes that the pod can be placed on based on requirements of the pods yaml. This list then gets sent to our Scheduler Extender , which is in charge of deciding which nodes can support the RDMA requirements specified in the pods yaml. The Scheduler Extender contacts each nodes RDMA Hardware Daemon Set , which returns a JSON formatted list of information about a nodes RDMA VF's back to the Scheduler Extender . The Scheduler Extender than processes all of the information to find a valid node that can meet the minimum bandwidth requirements of each of the requested interfaces that is specified in the pods yaml; the list of nodes whether blank or empty is sent back to the Kubernetes Core Scheduler. If no node is valid after the scheduling calls an error is raised and the pod is not placed, this error can be seen with a Kubernetes describe command of why the pod was not placed. Assuming that the pod was able to placed on at least one valid node, the Kubelet process on the valid node gets called to setup the pod. During the pods setup process the (CNI)[components.md#cni] is called to setup the network of the pod. The (CNI)[components.md#cni] first contacts the RDMA Hardware Daemon Set on the node that is running to get an up to date list of the state of the node. It then runs the same algorithm that the Scheduler Extender had run to find the correct placements of interfaces to meet the requirements of the bandwidth limitations. The (CNI)[components.md#cni] is atomic operation, so it either completes the setup of the pod or fails and rollbacks all changes made to any interfaces. Once the (CNI)[components.md#cni] finishes, the response is sent back to the Kubelet process of the node.","title":"Architecture"},{"location":"introduction/#limitations","text":"There are a couple limitations when it comes to our solution: - Mellanox Vendor - the following has only been test on a Mellanox Card - Data Plane Development Kit (DPDK) - the Mellanox solution may work with DPDK, it has not been tested with our solution (changes to (CNI)[https://github.com/rit-k8s-rdma/rit-k8s-rdma-sriov-cni] required) - Shared RDMA Device - the Mellanox solution may work with a shared RDMA interface, it has not been tested with our solution (changes to (CNI)[https://github.com/rit-k8s-rdma/rit-k8s-rdma-sriov-cni] required) - Dummy Device Plugin - the current solution requires the use of Device Plugin to give access to /dev/infiniband for open issues in Kubernets that can be found here and here . The main problem is Kubernetes does not have the ideas of a device directory that Docker has --device .","title":"Limitations"},{"location":"introduction/#future_work","text":"More Vendors - making the solution more interface driven so it can be adapted to more vendors then just Mellanox. Migrating CNI - the CNI is currently at an older version and we had to bootstrap the newer one, it should be upgraded. Scheduling - opening up the scheduler to be more adaptable to customizable scheduling algorithms.","title":"Future Work"}]}