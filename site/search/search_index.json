{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to rit-k8s-rdma docs \u00b6 This project aims to develop software components that allows the use of RDMA from within containers managed by Kubernetes. It differs from the previous projects it builds off of by allowing for more fine grained control over various aspects of the system, including the ability to set bandwidth limits and reserve bandwidth for specific Kubernetes pods. Introduction \u00b6 For a overview of the system, its design, and the problem it solves, see the introduction page . Components \u00b6 For a description of each of the software components that are part of the project, see the components page . Install \u00b6 For an installation guide, see the install page . Helpful Commands and Tools \u00b6 A list of useful tools and commands outside of this project is available on the helpful commands and tools page . Glossary \u00b6 For definitions of terms and acronyms, see the glossary page .","title":"Home"},{"location":"#welcome_to_rit-k8s-rdma_docs","text":"This project aims to develop software components that allows the use of RDMA from within containers managed by Kubernetes. It differs from the previous projects it builds off of by allowing for more fine grained control over various aspects of the system, including the ability to set bandwidth limits and reserve bandwidth for specific Kubernetes pods.","title":"Welcome to rit-k8s-rdma docs"},{"location":"#introduction","text":"For a overview of the system, its design, and the problem it solves, see the introduction page .","title":"Introduction"},{"location":"#components","text":"For a description of each of the software components that are part of the project, see the components page .","title":"Components"},{"location":"#install","text":"For an installation guide, see the install page .","title":"Install"},{"location":"#helpful_commands_and_tools","text":"A list of useful tools and commands outside of this project is available on the helpful commands and tools page .","title":"Helpful Commands and Tools"},{"location":"#glossary","text":"For definitions of terms and acronyms, see the glossary page .","title":"Glossary"},{"location":"components/","text":"Components \u00b6 The following are a detailed description of each of the components that make up the solution. RDMA Hardware Daemon Set \u00b6 The RDMA Harware Daemon Set is a Kubernetes Daemon Set that has two tasks. The first task is to initialize the RDMA SRIOV enabled interfaces on a node and the second task is to create a RESTful endpoint for providing metadata about the PF's and their associated VF's that exist on a given node that are part of the Kubernetes cluster. Further detail about each of the containers within the RDMA Hardware Daemon set can be seen below: Init Container - the init container is a privileged container that runs as a Kubernetes Init Container , in simplified terms this means it is run before any other container within the pod. The reason that the container needs to be run as a privileged container is because it will be working on a nodes network devices, which means it needs special access in order to configure them. The init container does two things, the first is to scan all the available interfaces on the node and determine if it is an RDMA device and whether SRIOV is enabled. For all interfaces that meet those two requirements, the init container configures each interfaces VF's to be available and ready to run. Server Container - the server container is an unprivileged container that scans through all available interfaces when starting up. It then makes a list of all interfaces that are SRIOV enabled and upon a RESTful get request from a user will return associated metadata about the container. The server container only scans the interfaces at startup because SRIOV devices can only change configurations upon a machine restart, which would rerun the init container and then the server container. The RESTful endpoint serves data in a JSON formatted list of PF's and each PF has an internal list of VF's, more info can be found here . The RESTful endpoint that the server container sets up is bound to the host network. Both the init container and the server container run under the same RDMA Hardware Daemon Set pod. When configuring how to install this pod please look at daemon set install instructions for more detail. Scheduler Extension \u00b6 The scheduler extension is a software component that is responsible for ensuring that RDMA-enabled pods are deployed onto nodes with enough RDMA resources (bandwidth and virtual functions) to support them. The scheduler extension runs alongside the kube-scheduler process within a Kubernetes cluster, and listens for HTTP requests on a TCP port (8888 by default). The extension is then registered within the configuration file for kube-scheduler (see the install page for details). Every time a new pod is being scheduled, kube-scheduler will make an HTTP request to the scheduler extenstion that contains the details of the new pod as well as a list of nodes within the cluster that kube-scheduler believe to be eligible to deploy the pod onto. The scheduler extension's job is to filter down that list based on whether the nodes in it have enough free RDMA VFs and bandwidth to support the pods requirements or not. In order to accomplish this, the scheduler extension contacts the RDMA hardware daemon set running on each of the potential nodes in the cluster to gather information about the current allocation of RDMA resources on that node. The requests to each daemon set are made in parallel, with a timeout for cases where a node's daemon set doesn't respond. Once the information has been gathered from a node, the scheduler extension calculates whether or not the node's interface and reserved bandwidth requirements can be met by the remaining resources on that node. If they can be, the node is added to a list of those that are elligible to deploy the pod onto. Once all of the nodes in the list passed in by kube-scheduler have been contacted or have timed out, this list is returned to kube-scheduler in an HTTP response. From here, kube-scheduler will limit its choice of where to place the new pod to one of the nodes in the returned list. If the returned list is empty, the pod will not be scheduled, and the output of kubectl describe for the pod will show the reasons given by the scheduler extension as to why the nodes in the list that was passed in were not eligible to host the pod. For help installing the scheduler extension and registering it with kube-scheduler, see the scheduler extension section of the install page. More information about the Kubernetes API for scheduler extensions can be found on the Kubernetes website and github . CNI Plugin \u00b6 The container network interface, or CNI, is a specification that governs the way network access is provided to containers and containerized applications. Kubernetes utilizes CNI as its standard for allocating IP addresses and network interfaces to pods that have been deployed in a cluster. A CNI plugin is a piece of software that performs that allocation, along with any other additional setup. Since RDMA network interfaces require specific additional setup to configure bandwidth limits and reservations, as well as to select virtual functions such that a pod's bandwidth requirements are satisfied, a CNI plugin is necessary for handling RDMA-enabled pods. The CNI plugin developed for this project is a fork of the existing Mellanox CNI plugin , which was limited in the fact that it always allocated one interface to each pod and didn't support bandwidth reservation or limitation. The CNI plugin for this project improves upon this by adding support for an arbitrary number of RDMA interfaces per pod, including the ability to allocate no RDMA interfaces to pods that do not need any. The CNI plugin is executed each time a pod is created or destroyed. It runs only once per pod for each of these actions, and must allocate or deallocate all of the interfaces for a pod at one time. This is done by executing an algorithm that finds a mapping of requested pod interfaces onto virtual functions that allows a pod's requirements to be satisfied. This is similar to one of the steps performed by the scheduler extension, though the scheduler extension need only determine whether a pod's requirements can be satisfied by a node, rather than what the exact final allocation of VFs to that pod will be to satisfy it. The tasks performed by the CNI plugin during pod setup are the following: Determine a placement of RDMA virtual functions that will satisfy the pod's requirements. Move each of the needed virtual functions into the pod's network namespace. Rename each of the virtual functions that have been added to the pod's network namespace. Set the bandwidth limits and reservations on each RDMA VF, if necessary. Allocate IP addresses to each of the VFs that have been allocated to the pod. The tasks performed by the CNI plugin during pod teardown are the following: Rename each of the virtual functions in the pod's network namespace. Move each of the virtual functions from the pod's network namespace back to the host's network namespace. Remove any bandwidth reservations or limitations set on the deallocated virtual functions. Dummy Device Plugin \u00b6 The Dummy Device Plugin is a stop gap measure for the current system. The directory /dev/infiniband is needed within any pod that requires RDMA. In order to access devices in this directory a container either needs to be run in privileged mode or a Device Plugin can return a directory that a container will have privileged access to. For obvious reasons we opted to create a Device Plugin for the sole purpose that it will give any container that requests an RDMA device the proper access to /dev/infiniband . This Dummy Device Plugin does not act like any other Device Plugin where it is meant to manage a specialized resources; that is done by the RDMA Hardware Daemon Set, Scheduler Extender, and CNI components. When configuring how to install this pod please look at dummy device plugin installation for more detail.","title":"Components"},{"location":"components/#components","text":"The following are a detailed description of each of the components that make up the solution.","title":"Components"},{"location":"components/#rdma_hardware_daemon_set","text":"The RDMA Harware Daemon Set is a Kubernetes Daemon Set that has two tasks. The first task is to initialize the RDMA SRIOV enabled interfaces on a node and the second task is to create a RESTful endpoint for providing metadata about the PF's and their associated VF's that exist on a given node that are part of the Kubernetes cluster. Further detail about each of the containers within the RDMA Hardware Daemon set can be seen below: Init Container - the init container is a privileged container that runs as a Kubernetes Init Container , in simplified terms this means it is run before any other container within the pod. The reason that the container needs to be run as a privileged container is because it will be working on a nodes network devices, which means it needs special access in order to configure them. The init container does two things, the first is to scan all the available interfaces on the node and determine if it is an RDMA device and whether SRIOV is enabled. For all interfaces that meet those two requirements, the init container configures each interfaces VF's to be available and ready to run. Server Container - the server container is an unprivileged container that scans through all available interfaces when starting up. It then makes a list of all interfaces that are SRIOV enabled and upon a RESTful get request from a user will return associated metadata about the container. The server container only scans the interfaces at startup because SRIOV devices can only change configurations upon a machine restart, which would rerun the init container and then the server container. The RESTful endpoint serves data in a JSON formatted list of PF's and each PF has an internal list of VF's, more info can be found here . The RESTful endpoint that the server container sets up is bound to the host network. Both the init container and the server container run under the same RDMA Hardware Daemon Set pod. When configuring how to install this pod please look at daemon set install instructions for more detail.","title":"RDMA Hardware Daemon Set"},{"location":"components/#scheduler_extension","text":"The scheduler extension is a software component that is responsible for ensuring that RDMA-enabled pods are deployed onto nodes with enough RDMA resources (bandwidth and virtual functions) to support them. The scheduler extension runs alongside the kube-scheduler process within a Kubernetes cluster, and listens for HTTP requests on a TCP port (8888 by default). The extension is then registered within the configuration file for kube-scheduler (see the install page for details). Every time a new pod is being scheduled, kube-scheduler will make an HTTP request to the scheduler extenstion that contains the details of the new pod as well as a list of nodes within the cluster that kube-scheduler believe to be eligible to deploy the pod onto. The scheduler extension's job is to filter down that list based on whether the nodes in it have enough free RDMA VFs and bandwidth to support the pods requirements or not. In order to accomplish this, the scheduler extension contacts the RDMA hardware daemon set running on each of the potential nodes in the cluster to gather information about the current allocation of RDMA resources on that node. The requests to each daemon set are made in parallel, with a timeout for cases where a node's daemon set doesn't respond. Once the information has been gathered from a node, the scheduler extension calculates whether or not the node's interface and reserved bandwidth requirements can be met by the remaining resources on that node. If they can be, the node is added to a list of those that are elligible to deploy the pod onto. Once all of the nodes in the list passed in by kube-scheduler have been contacted or have timed out, this list is returned to kube-scheduler in an HTTP response. From here, kube-scheduler will limit its choice of where to place the new pod to one of the nodes in the returned list. If the returned list is empty, the pod will not be scheduled, and the output of kubectl describe for the pod will show the reasons given by the scheduler extension as to why the nodes in the list that was passed in were not eligible to host the pod. For help installing the scheduler extension and registering it with kube-scheduler, see the scheduler extension section of the install page. More information about the Kubernetes API for scheduler extensions can be found on the Kubernetes website and github .","title":"Scheduler Extension"},{"location":"components/#cni_plugin","text":"The container network interface, or CNI, is a specification that governs the way network access is provided to containers and containerized applications. Kubernetes utilizes CNI as its standard for allocating IP addresses and network interfaces to pods that have been deployed in a cluster. A CNI plugin is a piece of software that performs that allocation, along with any other additional setup. Since RDMA network interfaces require specific additional setup to configure bandwidth limits and reservations, as well as to select virtual functions such that a pod's bandwidth requirements are satisfied, a CNI plugin is necessary for handling RDMA-enabled pods. The CNI plugin developed for this project is a fork of the existing Mellanox CNI plugin , which was limited in the fact that it always allocated one interface to each pod and didn't support bandwidth reservation or limitation. The CNI plugin for this project improves upon this by adding support for an arbitrary number of RDMA interfaces per pod, including the ability to allocate no RDMA interfaces to pods that do not need any. The CNI plugin is executed each time a pod is created or destroyed. It runs only once per pod for each of these actions, and must allocate or deallocate all of the interfaces for a pod at one time. This is done by executing an algorithm that finds a mapping of requested pod interfaces onto virtual functions that allows a pod's requirements to be satisfied. This is similar to one of the steps performed by the scheduler extension, though the scheduler extension need only determine whether a pod's requirements can be satisfied by a node, rather than what the exact final allocation of VFs to that pod will be to satisfy it. The tasks performed by the CNI plugin during pod setup are the following: Determine a placement of RDMA virtual functions that will satisfy the pod's requirements. Move each of the needed virtual functions into the pod's network namespace. Rename each of the virtual functions that have been added to the pod's network namespace. Set the bandwidth limits and reservations on each RDMA VF, if necessary. Allocate IP addresses to each of the VFs that have been allocated to the pod. The tasks performed by the CNI plugin during pod teardown are the following: Rename each of the virtual functions in the pod's network namespace. Move each of the virtual functions from the pod's network namespace back to the host's network namespace. Remove any bandwidth reservations or limitations set on the deallocated virtual functions.","title":"CNI Plugin"},{"location":"components/#dummy_device_plugin","text":"The Dummy Device Plugin is a stop gap measure for the current system. The directory /dev/infiniband is needed within any pod that requires RDMA. In order to access devices in this directory a container either needs to be run in privileged mode or a Device Plugin can return a directory that a container will have privileged access to. For obvious reasons we opted to create a Device Plugin for the sole purpose that it will give any container that requests an RDMA device the proper access to /dev/infiniband . This Dummy Device Plugin does not act like any other Device Plugin where it is meant to manage a specialized resources; that is done by the RDMA Hardware Daemon Set, Scheduler Extender, and CNI components. When configuring how to install this pod please look at dummy device plugin installation for more detail.","title":"Dummy Device Plugin"},{"location":"glossary/","text":"Glossary \u00b6 Glossary stuff... RDMA Kubernets SR-IOV VF PF Pod Container Namespace Network Filesystem Daemon Set","title":"Glossary"},{"location":"glossary/#glossary","text":"Glossary stuff... RDMA Kubernets SR-IOV VF PF Pod Container Namespace Network Filesystem Daemon Set","title":"Glossary"},{"location":"helpAndTools/","text":"Helpful Commands and Tools \u00b6","title":"Helpful Commands and Tools"},{"location":"helpAndTools/#helpful_commands_and_tools","text":"","title":"Helpful Commands and Tools"},{"location":"install/","text":"Install \u00b6 Prerequisites \u00b6 Before installing this system, you should have a working Kubernetes cluster set up. The following prerequisites must be met: - Kubernetes version 1.13 - Golang version 1.12 - Version 1.12 or greater is necessary to compile the software components of the system. - Mellanox OFED version 4.6-1.0.1.1 or greater - The firmware on Mellanox RDMA cards should be updated to the latest available version. Hardware Setup \u00b6 This section covers the configuration of Mellanox RDMA hardware in preparation for using SR-IOV. Enable SR-IOV in the BIOS of an machine with RDMA hardware installed. Load the Mellanox driver modules for making configuration changes to the RDMA hardware. Run: sudo mst start You should expect to see output similar to the following: Starting MST (Mellanox Software Tools) driver set Loading MST PCI module - Success Loading MST PCI configuration module - Success Create devices Unloading MST PCI module (unused) - Success Determine the path to the PCI device for the RDMA hardware card. Run: sudo mst status You should expect to see output similar to the following: MST modules: ------------ MST PCI module is not loaded MST PCI configuration module loaded MST devices: ------------ /dev/mst/mt4119_pciconf0 - PCI configuration cycles access. domain:bus:dev.fn=0000:04:00.0 addr.reg=88 data.reg=92 Chip revision is: 00 The /dev/mst/ path is the path to the device: /dev/mst/mt4119_pciconf0 Query the status of the device to determine whether SR-IOV is enabled, and how many virtual functions are configured. Run: mlxconfig -d <pci_device_path> q Here, <pci_device_path> is the path to the PCI device determined in the previous step. Ex: /dev/mst/mt4119_pciconf0 You should expect to see output similar to the following: Device #1: ---------- Device type: ConnectX5 Name: MCX556A-ECA_Ax Description: ConnectX-5 VPI adapter card; EDR IB (100Gb/s) and 100GbE; dual-port QSFP28; PCIe3.0 x16; tall bracket; ROHS R6 Device: /dev/mst/mt4119_pciconf0 Configurations: Next Boot MEMIC_BAR_SIZE 0 MEMIC_SIZE_LIMIT _256KB(1) HOST_CHAINING_MODE DISABLED(0) ... NUM_OF_VFS 120 ... SRIOV_EN True(1) ... The lines of interest to us are: SRIOV_EN True(1) Which indicates whether or not SR-IOV has been enabled on the RDMA card. And: NUM_OF_VFS 120 Which indicates how many SR-IOV virtual functions have been configured on the card. We want to ensure that SR-IOV is enabled, and the number of virtual functions is configured to the largest amount the card will support. Enable SR-IOV and confuigure number of VFs. Run: mlxconfig -d <pci_device_path> set SRIOV_EN=1 NUM_OF_VFS=<max_num_vfs> Here, <pci_device_path> is the path determined in step 3, and <max_num_vfs> is the highest number of virtual functions that the RDMA hardware card supports. This can be found in the documentation for that card (this can typically be found in the firmware manual). For example: mlxconfig -d /dev/mst/mt4119_pciconf0 set SRIOV_EN=1 NUM_OF_VFS=120 Choose 'yes' when asked whether to apply the configuration. Reboot the machine. Verify that the modification worked correctly. Run: sudo mst start sudo mst status mlxconfig -d <pci_device_path> q | egrep 'NUM_OF_VFS|SRIOV_EN' Ensure that the output of the last command matches the changes you have made prior to rebooting. RDMA Hardware Daemon Set \u00b6 This section covers the installation of the RDMA Hardware Daemon Set onto all of the worker nodes in the Kubernetes cluster. Use Kubernetes to deploy the Daemon Set to all the nodes in the cluster. On the master node of the Kubernetes cluster, run: kubectl apply -f <rdma_daemonset_yaml> Where <rdma_daemonset_yaml> is a YAML file that specifies the details of the Daemon Set. This file can be found at: https://github.com/rit-k8s-rdma/rit-k8s-rdma-ds/blob/master/rdma-ds.yaml Applying this Daemon Set configuration on the Kubernetes cluster will cause each worker node to pull container images built as part of this project from our Docker Hub repository . If you would like to build this Docker image yourself, the instructions are available within the RDMA Hardware Daemon Set repository . Verify that the RDMA Hardware Daemon Set is running on each worker node in the cluster. On the master node of the Kubernetes cluster, run: kubectl get pods -o wide --namespace kube-system Within the output for this command, you should see several lines with the name: rdma-ds-* (one for each worker node in the cluster). The status column of each of these pods will show Init:0/1 while the pod is performing hardware enumeration and initialization of the SR-IOV enabled RDMA hardware. One this has completed (it may take several minutes if there are a large number of virtual functions configured on a host), the status of the pods should switch to Running . Scheduler Extension \u00b6 This section covers the installation of the Scheduler Extension component. Install and run the scheduler extension Docker container on the master node of the Kubernetes cluster. Run docker run -d --rm --name ritk8srdma-scheduler-entension --network host ritk8srdma/rit-k8s-rdma-scheduler-extender This will pull the Docker image for the scheduler extension from our Docker Hub repository and run it. If you would like to build the scheduler extension docker image yourself, the instructions are available within the scheduler extension repository . Modify the configuration of the core Kubernetes scheduler to register the scheduler extension. On the master node of the Kubernetes cluster, edit or add the file /etc/kubernetes/scheduler-policy-config.json to register the scheduler extension. The following entry should be added to the 'extenders' list within that file: { \"urlPrefix\": \"http://127.0.0.1:8888/scheduler\", \"filterVerb\": \"rdma_scheduling\", \"bindVerb\": \"\", \"enableHttps\": false, \"nodeCacheCapable\": false, \"ignorable\": false } Here, the IP address/port combination of 127.0.0.1 and 8888 is used because the scheduler extension is running on the same node as the core Kubernetes scheduler (the master node of the Kubernetes cluster), and listening on port 8888. If the extension is run elsewhere or listening on a different port, the 'urlPrefix' parameter should be editted accordingly. An example version of this file is available in the scheduler extension repository . Ensure the core Kubernetes scheduler is using the configuration file where the scheduler extension is registered. Edit the file /etc/kubernetes/manifests/kube-scheduler.yaml on the master node of the Kubernetes cluster. Add the following volume to the pod definition if it does not exist (place the definition within the existing 'volumes' section if one exists): volumes: - hostPath: path: /etc/kubernetes/scheduler-policy-config.json type: FileOrCreate name: scheduler-policy-config Add the following directive to the command to be run in the kube-scheduler container: --policy-config-file=/etc/kubernetes/scheduler-policy-config.json Overall, the whole file should look like: apiVersion: v1 kind: Pod metadata: annotations: scheduler.alpha.kubernetes.io/critical-pod: \"\" creationTimestamp: null labels: component: kube-scheduler tier: control-plane name: kube-scheduler namespace: kube-system spec: containers: - command: - kube-scheduler - --address=127.0.0.1 - --kubeconfig=/etc/kubernetes/scheduler.conf - --policy-config-file=/etc/kubernetes/scheduler-policy-config.json - --leader-elect=true image: k8s.gcr.io/kube-scheduler:v1.13.5 imagePullPolicy: IfNotPresent livenessProbe: failureThreshold: 8 httpGet: host: 127.0.0.1 path: /healthz port: 10251 scheme: HTTP initialDelaySeconds: 15 timeoutSeconds: 15 name: kube-scheduler resources: requests: cpu: 100m volumeMounts: - mountPath: /etc/kubernetes/scheduler.conf name: kubeconfig readOnly: true - mountPath: /etc/kubernetes/scheduler-policy-config.json name: scheduler-policy-config readOnly: true hostNetwork: true priorityClassName: system-cluster-critical volumes: - hostPath: path: /etc/kubernetes/scheduler.conf type: FileOrCreate name: kubeconfig - hostPath: path: /etc/kubernetes/scheduler-policy-config.json type: FileOrCreate name: scheduler-policy-config status: {} Ensure that the scheduler extension has started up correctly. Run docker logs ritk8srdma-scheduler-entension on the node where the scheduler extension is running. The output should include the following line at the top of its output: YYYY/MM/DD HH:MM:SS RDMA scheduler extender listening on port: <port_number> This command can be run whenever necessary to view the logging output from the scheduler extension. CNI Plugin \u00b6 This section covers the installation of the CNI plugin on each RDMA-enabled worker node in the Kubernetes cluster. Install the CNI plugin executable. Copy the 'rit-k8s-rdma-cni-linux-amd64' executable from the releases page of the repository . Place it in /opt/cni/bin/ on each RDMA-enabled worker node in the Kubernetes cluster. Install the CNI plugin configuration file. Copy the file '10-ritk8srdma-cni.conf' from the releases page of the repository . Place it in /etc/cni/net.d/ on each RDMA-enabled worker node in the Kubernetes cluster. This configuration file can be edited to fit the needs of your environment. Ensure that this file is the first one (lexicographically) within that directory. Kubernetes always uses the CNI configuration that comes first lexicographically within this directory. Within this file, the 'type' parameter specifies the name of the CNI executable that will be run when a pod is deployed. This name should match the name of the executable installed during step 1. To compile the CNI plugin binary yourself, checkout the CNI repository . Enter the 'sriov' directory within the checked-out copy of the repository, then run go install . The binary should then be created in the appropriate Golang bin directory. Dummy Device Plugin \u00b6 This section covers the installation of the Dummy Device Plugin onto all of the worker nodes in the Kubernetes cluster. Use Kubernetes to deploy the dummy device plugin to all the nodes in the cluster. On the master node of the Kubernetes cluster, run: kubectl apply -f <dummy_plugin_yaml> Where <dummy_plugin_yaml> is a YAML file that specifies the details of the Dummy Device Plugin. This file can be found at: https://github.com/rit-k8s-rdma/rit-k8s-rdma-dummy-device-plugin/blob/master/rdma-dummy-dp-ds.yaml Applying this configuration on the Kubernetes cluster will cause each worker node to pull container images built as part of this project from our Docker Hub repository . These images contain the files necessary to run the Dummy Device Plugin. If you would like to build the Dummy Device Plugin Docker image yourself, the instructions are available within the Dummy Device Plugin repository . Verify that the Dummy Device Plugin is running on each worker node in the cluster. On the master node of the Kubernetes cluster, run: kubectl get pods -o wide --namespace kube-system Within the output for this command, you should see several lines with the name: rdma-dummy-dp-ds-* (one for each worker node in the cluster). The status column of each of these pods should show Running . Pod YAML Changes \u00b6 To take advantage of RDMA within a Kubernetes pod, that pod's definition (YAML file) will need to be updated to specify the RDMA interfaces that it requires. This involves the following steps: Add the rdma_interfaces_required directive to the pod's metadata annotations: apiVersion: v1 kind: Pod metadata: name: pod1 annotations: rdma_interfaces_required: '[ {\"min_tx_rate\": 15000, \"max_tx_rate\": 20000}, {\"min_tx_rate\": 5000}, {} ]' spec: ... The value of this annotation should be a JSON-formatted string that contains a list of RDMA interfaces needed by the pod, as well as the bandwidth limitations and reservations for each of those interfaces. In this case min_tx_rate specifies an amount of bandwidth that should be reserved for the pod to use exclusively through a specific RDMA interface, while max_tx_rate sets a cap on the amount of bandwidth that can used by a pod through an interface. Either or both of these properties can be omitted if you do not need a bandwidth cap/reservation. In the example above, three RDMA interfaces are requested by a pod: the first sets both properties, the second has only a bandwidth reservation, and the third has no limit nor any reserved bandwidth. The numbers used are in units of megabits of bandwidth per second (Mb/S). Add a request for a /dev/infiniband/ mount to each container that will need access to RDMA interfaces: ... containers: - image: mellanoxubuntudocker:latest name: mofed-test-ctr1 resources: limits: rdma-sriov/dev-infiniband-mount: 1 ... The line rdma-sriov/dev-infiniband-mount: 1 indicates that the container requires privleged access to the /dev/infiniband directory. The quantity specified should be 1 (technically, the dummy device plugin is advertising an infinite amount of this resource type, but only one is needed to provide the mount point to the container). Make sure the container images being deployed in the pod contain the necessary RDMA libraries. This can be done by utilizing Mellanox's 'mellanoxubuntudocker:latest' Docker container image (and/or using this as a base to build other containers). An example of a complete pod configuration file is available from our common repository . Testing and Verification \u00b6 Deploy a Pod \u00b6 -deploy pod -kubectl get pods -if status != running: -kubectl describe pod -possible errors: -scheduler extension not running/found -see logs for kube-scheduler & scheduler extension itself -pod's rdma_interfaces_required JSON is malformatted -fix it -nodes in cluster did not respond to scheduler extension's requests -see scheduler extension's logs -make sure scheduler extension is contacting DaemonSet on same port daemonset is listening -use web browser/curl to make sure daemonset is working -nodes in cluster did not have enough free bandwidth -check response from daemonset using curl/browser -check how much bandwidth/how many interfaces the pod is requesting Test connectivity between two pods \u00b6 -have two pods w/ node selectors and run ib_send_bw between them","title":"Install"},{"location":"install/#install","text":"","title":"Install"},{"location":"install/#prerequisites","text":"Before installing this system, you should have a working Kubernetes cluster set up. The following prerequisites must be met: - Kubernetes version 1.13 - Golang version 1.12 - Version 1.12 or greater is necessary to compile the software components of the system. - Mellanox OFED version 4.6-1.0.1.1 or greater - The firmware on Mellanox RDMA cards should be updated to the latest available version.","title":"Prerequisites"},{"location":"install/#hardware_setup","text":"This section covers the configuration of Mellanox RDMA hardware in preparation for using SR-IOV. Enable SR-IOV in the BIOS of an machine with RDMA hardware installed. Load the Mellanox driver modules for making configuration changes to the RDMA hardware. Run: sudo mst start You should expect to see output similar to the following: Starting MST (Mellanox Software Tools) driver set Loading MST PCI module - Success Loading MST PCI configuration module - Success Create devices Unloading MST PCI module (unused) - Success Determine the path to the PCI device for the RDMA hardware card. Run: sudo mst status You should expect to see output similar to the following: MST modules: ------------ MST PCI module is not loaded MST PCI configuration module loaded MST devices: ------------ /dev/mst/mt4119_pciconf0 - PCI configuration cycles access. domain:bus:dev.fn=0000:04:00.0 addr.reg=88 data.reg=92 Chip revision is: 00 The /dev/mst/ path is the path to the device: /dev/mst/mt4119_pciconf0 Query the status of the device to determine whether SR-IOV is enabled, and how many virtual functions are configured. Run: mlxconfig -d <pci_device_path> q Here, <pci_device_path> is the path to the PCI device determined in the previous step. Ex: /dev/mst/mt4119_pciconf0 You should expect to see output similar to the following: Device #1: ---------- Device type: ConnectX5 Name: MCX556A-ECA_Ax Description: ConnectX-5 VPI adapter card; EDR IB (100Gb/s) and 100GbE; dual-port QSFP28; PCIe3.0 x16; tall bracket; ROHS R6 Device: /dev/mst/mt4119_pciconf0 Configurations: Next Boot MEMIC_BAR_SIZE 0 MEMIC_SIZE_LIMIT _256KB(1) HOST_CHAINING_MODE DISABLED(0) ... NUM_OF_VFS 120 ... SRIOV_EN True(1) ... The lines of interest to us are: SRIOV_EN True(1) Which indicates whether or not SR-IOV has been enabled on the RDMA card. And: NUM_OF_VFS 120 Which indicates how many SR-IOV virtual functions have been configured on the card. We want to ensure that SR-IOV is enabled, and the number of virtual functions is configured to the largest amount the card will support. Enable SR-IOV and confuigure number of VFs. Run: mlxconfig -d <pci_device_path> set SRIOV_EN=1 NUM_OF_VFS=<max_num_vfs> Here, <pci_device_path> is the path determined in step 3, and <max_num_vfs> is the highest number of virtual functions that the RDMA hardware card supports. This can be found in the documentation for that card (this can typically be found in the firmware manual). For example: mlxconfig -d /dev/mst/mt4119_pciconf0 set SRIOV_EN=1 NUM_OF_VFS=120 Choose 'yes' when asked whether to apply the configuration. Reboot the machine. Verify that the modification worked correctly. Run: sudo mst start sudo mst status mlxconfig -d <pci_device_path> q | egrep 'NUM_OF_VFS|SRIOV_EN' Ensure that the output of the last command matches the changes you have made prior to rebooting.","title":"Hardware Setup"},{"location":"install/#rdma_hardware_daemon_set","text":"This section covers the installation of the RDMA Hardware Daemon Set onto all of the worker nodes in the Kubernetes cluster. Use Kubernetes to deploy the Daemon Set to all the nodes in the cluster. On the master node of the Kubernetes cluster, run: kubectl apply -f <rdma_daemonset_yaml> Where <rdma_daemonset_yaml> is a YAML file that specifies the details of the Daemon Set. This file can be found at: https://github.com/rit-k8s-rdma/rit-k8s-rdma-ds/blob/master/rdma-ds.yaml Applying this Daemon Set configuration on the Kubernetes cluster will cause each worker node to pull container images built as part of this project from our Docker Hub repository . If you would like to build this Docker image yourself, the instructions are available within the RDMA Hardware Daemon Set repository . Verify that the RDMA Hardware Daemon Set is running on each worker node in the cluster. On the master node of the Kubernetes cluster, run: kubectl get pods -o wide --namespace kube-system Within the output for this command, you should see several lines with the name: rdma-ds-* (one for each worker node in the cluster). The status column of each of these pods will show Init:0/1 while the pod is performing hardware enumeration and initialization of the SR-IOV enabled RDMA hardware. One this has completed (it may take several minutes if there are a large number of virtual functions configured on a host), the status of the pods should switch to Running .","title":"RDMA Hardware Daemon Set"},{"location":"install/#scheduler_extension","text":"This section covers the installation of the Scheduler Extension component. Install and run the scheduler extension Docker container on the master node of the Kubernetes cluster. Run docker run -d --rm --name ritk8srdma-scheduler-entension --network host ritk8srdma/rit-k8s-rdma-scheduler-extender This will pull the Docker image for the scheduler extension from our Docker Hub repository and run it. If you would like to build the scheduler extension docker image yourself, the instructions are available within the scheduler extension repository . Modify the configuration of the core Kubernetes scheduler to register the scheduler extension. On the master node of the Kubernetes cluster, edit or add the file /etc/kubernetes/scheduler-policy-config.json to register the scheduler extension. The following entry should be added to the 'extenders' list within that file: { \"urlPrefix\": \"http://127.0.0.1:8888/scheduler\", \"filterVerb\": \"rdma_scheduling\", \"bindVerb\": \"\", \"enableHttps\": false, \"nodeCacheCapable\": false, \"ignorable\": false } Here, the IP address/port combination of 127.0.0.1 and 8888 is used because the scheduler extension is running on the same node as the core Kubernetes scheduler (the master node of the Kubernetes cluster), and listening on port 8888. If the extension is run elsewhere or listening on a different port, the 'urlPrefix' parameter should be editted accordingly. An example version of this file is available in the scheduler extension repository . Ensure the core Kubernetes scheduler is using the configuration file where the scheduler extension is registered. Edit the file /etc/kubernetes/manifests/kube-scheduler.yaml on the master node of the Kubernetes cluster. Add the following volume to the pod definition if it does not exist (place the definition within the existing 'volumes' section if one exists): volumes: - hostPath: path: /etc/kubernetes/scheduler-policy-config.json type: FileOrCreate name: scheduler-policy-config Add the following directive to the command to be run in the kube-scheduler container: --policy-config-file=/etc/kubernetes/scheduler-policy-config.json Overall, the whole file should look like: apiVersion: v1 kind: Pod metadata: annotations: scheduler.alpha.kubernetes.io/critical-pod: \"\" creationTimestamp: null labels: component: kube-scheduler tier: control-plane name: kube-scheduler namespace: kube-system spec: containers: - command: - kube-scheduler - --address=127.0.0.1 - --kubeconfig=/etc/kubernetes/scheduler.conf - --policy-config-file=/etc/kubernetes/scheduler-policy-config.json - --leader-elect=true image: k8s.gcr.io/kube-scheduler:v1.13.5 imagePullPolicy: IfNotPresent livenessProbe: failureThreshold: 8 httpGet: host: 127.0.0.1 path: /healthz port: 10251 scheme: HTTP initialDelaySeconds: 15 timeoutSeconds: 15 name: kube-scheduler resources: requests: cpu: 100m volumeMounts: - mountPath: /etc/kubernetes/scheduler.conf name: kubeconfig readOnly: true - mountPath: /etc/kubernetes/scheduler-policy-config.json name: scheduler-policy-config readOnly: true hostNetwork: true priorityClassName: system-cluster-critical volumes: - hostPath: path: /etc/kubernetes/scheduler.conf type: FileOrCreate name: kubeconfig - hostPath: path: /etc/kubernetes/scheduler-policy-config.json type: FileOrCreate name: scheduler-policy-config status: {} Ensure that the scheduler extension has started up correctly. Run docker logs ritk8srdma-scheduler-entension on the node where the scheduler extension is running. The output should include the following line at the top of its output: YYYY/MM/DD HH:MM:SS RDMA scheduler extender listening on port: <port_number> This command can be run whenever necessary to view the logging output from the scheduler extension.","title":"Scheduler Extension"},{"location":"install/#cni_plugin","text":"This section covers the installation of the CNI plugin on each RDMA-enabled worker node in the Kubernetes cluster. Install the CNI plugin executable. Copy the 'rit-k8s-rdma-cni-linux-amd64' executable from the releases page of the repository . Place it in /opt/cni/bin/ on each RDMA-enabled worker node in the Kubernetes cluster. Install the CNI plugin configuration file. Copy the file '10-ritk8srdma-cni.conf' from the releases page of the repository . Place it in /etc/cni/net.d/ on each RDMA-enabled worker node in the Kubernetes cluster. This configuration file can be edited to fit the needs of your environment. Ensure that this file is the first one (lexicographically) within that directory. Kubernetes always uses the CNI configuration that comes first lexicographically within this directory. Within this file, the 'type' parameter specifies the name of the CNI executable that will be run when a pod is deployed. This name should match the name of the executable installed during step 1. To compile the CNI plugin binary yourself, checkout the CNI repository . Enter the 'sriov' directory within the checked-out copy of the repository, then run go install . The binary should then be created in the appropriate Golang bin directory.","title":"CNI Plugin"},{"location":"install/#dummy_device_plugin","text":"This section covers the installation of the Dummy Device Plugin onto all of the worker nodes in the Kubernetes cluster. Use Kubernetes to deploy the dummy device plugin to all the nodes in the cluster. On the master node of the Kubernetes cluster, run: kubectl apply -f <dummy_plugin_yaml> Where <dummy_plugin_yaml> is a YAML file that specifies the details of the Dummy Device Plugin. This file can be found at: https://github.com/rit-k8s-rdma/rit-k8s-rdma-dummy-device-plugin/blob/master/rdma-dummy-dp-ds.yaml Applying this configuration on the Kubernetes cluster will cause each worker node to pull container images built as part of this project from our Docker Hub repository . These images contain the files necessary to run the Dummy Device Plugin. If you would like to build the Dummy Device Plugin Docker image yourself, the instructions are available within the Dummy Device Plugin repository . Verify that the Dummy Device Plugin is running on each worker node in the cluster. On the master node of the Kubernetes cluster, run: kubectl get pods -o wide --namespace kube-system Within the output for this command, you should see several lines with the name: rdma-dummy-dp-ds-* (one for each worker node in the cluster). The status column of each of these pods should show Running .","title":"Dummy Device Plugin"},{"location":"install/#pod_yaml_changes","text":"To take advantage of RDMA within a Kubernetes pod, that pod's definition (YAML file) will need to be updated to specify the RDMA interfaces that it requires. This involves the following steps: Add the rdma_interfaces_required directive to the pod's metadata annotations: apiVersion: v1 kind: Pod metadata: name: pod1 annotations: rdma_interfaces_required: '[ {\"min_tx_rate\": 15000, \"max_tx_rate\": 20000}, {\"min_tx_rate\": 5000}, {} ]' spec: ... The value of this annotation should be a JSON-formatted string that contains a list of RDMA interfaces needed by the pod, as well as the bandwidth limitations and reservations for each of those interfaces. In this case min_tx_rate specifies an amount of bandwidth that should be reserved for the pod to use exclusively through a specific RDMA interface, while max_tx_rate sets a cap on the amount of bandwidth that can used by a pod through an interface. Either or both of these properties can be omitted if you do not need a bandwidth cap/reservation. In the example above, three RDMA interfaces are requested by a pod: the first sets both properties, the second has only a bandwidth reservation, and the third has no limit nor any reserved bandwidth. The numbers used are in units of megabits of bandwidth per second (Mb/S). Add a request for a /dev/infiniband/ mount to each container that will need access to RDMA interfaces: ... containers: - image: mellanoxubuntudocker:latest name: mofed-test-ctr1 resources: limits: rdma-sriov/dev-infiniband-mount: 1 ... The line rdma-sriov/dev-infiniband-mount: 1 indicates that the container requires privleged access to the /dev/infiniband directory. The quantity specified should be 1 (technically, the dummy device plugin is advertising an infinite amount of this resource type, but only one is needed to provide the mount point to the container). Make sure the container images being deployed in the pod contain the necessary RDMA libraries. This can be done by utilizing Mellanox's 'mellanoxubuntudocker:latest' Docker container image (and/or using this as a base to build other containers). An example of a complete pod configuration file is available from our common repository .","title":"Pod YAML Changes"},{"location":"install/#testing_and_verification","text":"","title":"Testing and Verification"},{"location":"install/#deploy_a_pod","text":"-deploy pod -kubectl get pods -if status != running: -kubectl describe pod -possible errors: -scheduler extension not running/found -see logs for kube-scheduler & scheduler extension itself -pod's rdma_interfaces_required JSON is malformatted -fix it -nodes in cluster did not respond to scheduler extension's requests -see scheduler extension's logs -make sure scheduler extension is contacting DaemonSet on same port daemonset is listening -use web browser/curl to make sure daemonset is working -nodes in cluster did not have enough free bandwidth -check response from daemonset using curl/browser -check how much bandwidth/how many interfaces the pod is requesting","title":"Deploy a Pod"},{"location":"install/#test_connectivity_between_two_pods","text":"-have two pods w/ node selectors and run ib_send_bw between them","title":"Test connectivity between two pods"},{"location":"introduction/","text":"Introduction \u00b6 The main focus of this project was to enable bandwidth limiting and allow for multiple interfaces to be specified for RDMA interfaces on a per pod basis. The current Mellanox Solution has a number of pitfalls such as an inaccurate state between the CNI and the Device Plugin. The Device Plugin is managed by Kubernetes, so Kubernets decides which interface to allocate to a given container. This is not reflected accurately in the CNI which may give it a different interface. In the current solution a pod may have 3 containers that each request a single RDMA interface; in the solution the Device Plugin removes 3 RDMA interfaces from the available pool of interfaces, but the CNI only allocates a single interface; the state of the CNI and the Device Plugin are incorrect. One of the largest problems is that Mellanox treats RDMA interfaces as a container specified resource, when in reality network namespaces are shared across pods, so any container in a pod has access to all of the same interfaces. This becomes a problem because when specifying Device Resources within a pod yaml, they are on a per container basis. For all of the reasons above the Device Plugin approach was abandoned because it could not accommodate our goals and instead we opted for a new architecture. Architecture \u00b6 The main system architecture for our design can be seen below; the green color specifies our components for our design and the yellow color specify Kubernetes components: The main workflow for how a pod would deployed in our system begins with a request for deploying a pod going to the master nodes Kubernetes Control Process. After that request is seen the Kubernetes Scheduler creates a list of possible nodes that the pod can be placed on based on requirements of the pods yaml. This list then gets sent to our Scheduler Extension , which is in charge of deciding which nodes can support the RDMA requirements specified in the pods yaml. The Scheduler Extension contacts each nodes RDMA Hardware Daemon Set , which returns a JSON formatted list of information about a nodes RDMA VF's back to the Scheduler Extension . The Scheduler Extension than processes all of the information to find a valid node that can meet the minimum bandwidth requirements of each of the requested interfaces that is specified in the pods yaml; the list of nodes whether blank or empty is sent back to the Kubernetes Core Scheduler. If no node is valid after the scheduling calls an error is raised and the pod is not placed, this error can be seen with a Kubernetes describe command of why the pod was not placed. Assuming that the pod was able to placed on at least one valid node, the Kubelet process on the valid node gets called to setup the pod. During the pods setup process the (CNI)[components.md#cni] is called to setup the network of the pod. The (CNI)[components.md#cni] first contacts the RDMA Hardware Daemon Set on the node that is running to get an up to date list of the state of the node. It then runs the same algorithm that the Scheduler Extension had run to find the correct placements of interfaces to meet the requirements of the bandwidth limitations. The (CNI)[components.md#cni] is atomic operation, so it either completes the setup of the pod or fails and rollbacks all changes made to any interfaces. Once the (CNI)[components.md#cni] finishes, the response is sent back to the Kubelet process of the node. Limitations \u00b6 There are a couple limitations when it comes to our solution: - Mellanox Vendor - the following has only been test on a Mellanox Card - Data Plane Development Kit (DPDK) - the Mellanox solution may work with DPDK, it has not been tested with our solution (changes to (CNI)[https://github.com/rit-k8s-rdma/rit-k8s-rdma-sriov-cni] required) - Shared RDMA Device - the Mellanox solution may work with a shared RDMA interface, it has not been tested with our solution (changes to (CNI)[https://github.com/rit-k8s-rdma/rit-k8s-rdma-sriov-cni] required) - Dummy Device Plugin - the current solution requires the use of Device Plugin to give access to /dev/infiniband for open issues in Kubernets that can be found here and here . The main problem is Kubernetes does not have the ideas of a device directory that Docker has --device . Future Work \u00b6 More Vendors - making the solution more interface driven so it can be adapted to more vendors then just Mellanox. Migrating CNI - the CNI is currently at an older version and we had to bootstrap the newer one, it should be upgraded. Scheduling - opening up the scheduler to be more adaptable to customizable scheduling algorithms.","title":"Introduction"},{"location":"introduction/#introduction","text":"The main focus of this project was to enable bandwidth limiting and allow for multiple interfaces to be specified for RDMA interfaces on a per pod basis. The current Mellanox Solution has a number of pitfalls such as an inaccurate state between the CNI and the Device Plugin. The Device Plugin is managed by Kubernetes, so Kubernets decides which interface to allocate to a given container. This is not reflected accurately in the CNI which may give it a different interface. In the current solution a pod may have 3 containers that each request a single RDMA interface; in the solution the Device Plugin removes 3 RDMA interfaces from the available pool of interfaces, but the CNI only allocates a single interface; the state of the CNI and the Device Plugin are incorrect. One of the largest problems is that Mellanox treats RDMA interfaces as a container specified resource, when in reality network namespaces are shared across pods, so any container in a pod has access to all of the same interfaces. This becomes a problem because when specifying Device Resources within a pod yaml, they are on a per container basis. For all of the reasons above the Device Plugin approach was abandoned because it could not accommodate our goals and instead we opted for a new architecture.","title":"Introduction"},{"location":"introduction/#architecture","text":"The main system architecture for our design can be seen below; the green color specifies our components for our design and the yellow color specify Kubernetes components: The main workflow for how a pod would deployed in our system begins with a request for deploying a pod going to the master nodes Kubernetes Control Process. After that request is seen the Kubernetes Scheduler creates a list of possible nodes that the pod can be placed on based on requirements of the pods yaml. This list then gets sent to our Scheduler Extension , which is in charge of deciding which nodes can support the RDMA requirements specified in the pods yaml. The Scheduler Extension contacts each nodes RDMA Hardware Daemon Set , which returns a JSON formatted list of information about a nodes RDMA VF's back to the Scheduler Extension . The Scheduler Extension than processes all of the information to find a valid node that can meet the minimum bandwidth requirements of each of the requested interfaces that is specified in the pods yaml; the list of nodes whether blank or empty is sent back to the Kubernetes Core Scheduler. If no node is valid after the scheduling calls an error is raised and the pod is not placed, this error can be seen with a Kubernetes describe command of why the pod was not placed. Assuming that the pod was able to placed on at least one valid node, the Kubelet process on the valid node gets called to setup the pod. During the pods setup process the (CNI)[components.md#cni] is called to setup the network of the pod. The (CNI)[components.md#cni] first contacts the RDMA Hardware Daemon Set on the node that is running to get an up to date list of the state of the node. It then runs the same algorithm that the Scheduler Extension had run to find the correct placements of interfaces to meet the requirements of the bandwidth limitations. The (CNI)[components.md#cni] is atomic operation, so it either completes the setup of the pod or fails and rollbacks all changes made to any interfaces. Once the (CNI)[components.md#cni] finishes, the response is sent back to the Kubelet process of the node.","title":"Architecture"},{"location":"introduction/#limitations","text":"There are a couple limitations when it comes to our solution: - Mellanox Vendor - the following has only been test on a Mellanox Card - Data Plane Development Kit (DPDK) - the Mellanox solution may work with DPDK, it has not been tested with our solution (changes to (CNI)[https://github.com/rit-k8s-rdma/rit-k8s-rdma-sriov-cni] required) - Shared RDMA Device - the Mellanox solution may work with a shared RDMA interface, it has not been tested with our solution (changes to (CNI)[https://github.com/rit-k8s-rdma/rit-k8s-rdma-sriov-cni] required) - Dummy Device Plugin - the current solution requires the use of Device Plugin to give access to /dev/infiniband for open issues in Kubernets that can be found here and here . The main problem is Kubernetes does not have the ideas of a device directory that Docker has --device .","title":"Limitations"},{"location":"introduction/#future_work","text":"More Vendors - making the solution more interface driven so it can be adapted to more vendors then just Mellanox. Migrating CNI - the CNI is currently at an older version and we had to bootstrap the newer one, it should be upgraded. Scheduling - opening up the scheduler to be more adaptable to customizable scheduling algorithms.","title":"Future Work"}]}