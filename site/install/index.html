<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  
  <title>Install - RIT k8s RDMA Documentation</title>
  

  <link rel="shortcut icon" href="../img/favicon.ico">

  
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="../css/highlight.css">

  
  <script>
    // Current page data
    var mkdocs_page_name = "Install";
    var mkdocs_page_input_path = "install.md";
    var mkdocs_page_url = "/install/";
  </script>
  
  <script src="../js/jquery-2.1.1.min.js"></script>
  <script src="../js/modernizr-2.8.3.min.js"></script>
  <script type="text/javascript" src="../js/highlight.pack.js"></script>
  <script src="../js/theme.js"></script> 

  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> RIT k8s RDMA Documentation</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
        <ul class="current">
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="..">Home</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../components/">Components</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../glossary/">Glossary</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../helpAndTools/">helpAndTools</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 current">
        <a class="current" href="./">Install</a>
        
            <ul>
            
                <li class="toctree-l3"><a href="#install">Install</a></li>
                
                    <li><a class="toctree-l4" href="#prerequisites">Prerequisites</a></li>
                
                    <li><a class="toctree-l4" href="#hardware_setup">Hardware Setup</a></li>
                
                    <li><a class="toctree-l4" href="#rdma_hardware_daemon_set">RDMA Hardware Daemon Set</a></li>
                
                    <li><a class="toctree-l4" href="#scheduler_extension">Scheduler Extension</a></li>
                
                    <li><a class="toctree-l4" href="#cni_plugin">CNI Plugin</a></li>
                
                    <li><a class="toctree-l4" href="#dummy_device_plugin">Dummy Device Plugin</a></li>
                
                    <li><a class="toctree-l4" href="#pod_yaml_changes">Pod YAML Changes</a></li>
                
                    <li><a class="toctree-l4" href="#testing_and_verification">Testing and Verification</a></li>
                
            
            </ul>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../introduction/">Introduction</a>
        
    </li>
<li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">RIT k8s RDMA Documentation</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
    
    <li>Install</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="install">Install<a class="headerlink" href="#install" title="Permanent link">&para;</a></h1>
<h2 id="prerequisites">Prerequisites<a class="headerlink" href="#prerequisites" title="Permanent link">&para;</a></h2>
<p>Before installing this system, you should have a working Kubernetes cluster set up. The following prerequisites must be met:
 - Kubernetes version 1.13
 - Golang version 1.12
   - Version 1.12 or greater is necessary to compile the software components of the system.
 - Mellanox OFED version 4.6-1.0.1.1 or greater
   - The firmware on Mellanox RDMA cards should be updated to the latest available version.</p>
<h2 id="hardware_setup">Hardware Setup<a class="headerlink" href="#hardware_setup" title="Permanent link">&para;</a></h2>
<p>This section covers the configuration of Mellanox RDMA hardware in preparation for using SR-IOV.</p>
<ol>
<li>Enable SR-IOV in the BIOS of an machine with RDMA hardware installed.</li>
<li>
<p>Load the Mellanox driver modules for making configuration changes to the RDMA hardware.</p>
<p>Run:</p>
<pre><code>sudo mst start
</code></pre>
<p>You should expect to see output similar to the following:</p>
<pre><code>Starting MST (Mellanox Software Tools) driver set
Loading MST PCI module - Success
Loading MST PCI configuration module - Success
Create devices
Unloading MST PCI module (unused) - Success
</code></pre>
</li>
<li>
<p>Determine the path to the PCI device for the RDMA hardware card.</p>
<p>Run:</p>
<pre><code>sudo mst status
</code></pre>
<p>You should expect to see output similar to the following:</p>
<pre><code>MST modules:
------------
    MST PCI module is not loaded
    MST PCI configuration module loaded

MST devices:
------------
/dev/mst/mt4119_pciconf0         - PCI configuration cycles access.
                                   domain:bus:dev.fn=0000:04:00.0 addr.reg=88 data.reg=92
                                   Chip revision is: 00
</code></pre>
<p>The /dev/mst/ path is the path to the device:</p>
<pre><code>/dev/mst/mt4119_pciconf0
</code></pre>
</li>
<li>
<p>Query the status of the device to determine whether SR-IOV is enabled, and how many virtual functions are configured.</p>
<p>Run:</p>
<pre><code>mlxconfig -d &lt;pci_device_path&gt; q
</code></pre>
<p>Here, &lt;pci_device_path> is the path to the PCI device determined in the previous step. Ex:</p>
<pre><code>/dev/mst/mt4119_pciconf0
</code></pre>
<p>You should expect to see output similar to the following:</p>
<pre><code>Device #1:
----------

Device type:    ConnectX5       
Name:           MCX556A-ECA_Ax  
Description:    ConnectX-5 VPI adapter card; EDR IB (100Gb/s) and 100GbE; dual-port QSFP28; PCIe3.0 x16; tall bracket; ROHS R6
Device:         /dev/mst/mt4119_pciconf0

Configurations:                              Next Boot
         MEMIC_BAR_SIZE                      0               
         MEMIC_SIZE_LIMIT                    _256KB(1)       
         HOST_CHAINING_MODE                  DISABLED(0)
         ...
         NUM_OF_VFS                          120
         ...
         SRIOV_EN                            True(1)
         ...
</code></pre>
<p>The lines of interest to us are:</p>
<pre><code>SRIOV_EN                            True(1)
</code></pre>
<p>Which indicates whether or not SR-IOV has been enabled on the RDMA card.</p>
<p>And:</p>
<pre><code>NUM_OF_VFS                          120
</code></pre>
<p>Which indicates how many SR-IOV virtual functions have been configured on the card.</p>
<p>We want to ensure that SR-IOV is enabled, and the number of virtual functions is configured to the largest amount the card will support.</p>
</li>
<li>
<p>Enable SR-IOV and confuigure number of VFs.</p>
<p>Run:</p>
<pre><code>mlxconfig -d &lt;pci_device_path&gt; set SRIOV_EN=1 NUM_OF_VFS=&lt;max_num_vfs&gt;
</code></pre>
<p>Here, &lt;pci_device_path> is the path determined in step 3, and &lt;max_num_vfs> is the highest number of virtual functions that the RDMA hardware card supports. This can be found in the documentation for that card (this can typically be found in the firmware manual).</p>
<p>For example:</p>
<pre><code>mlxconfig -d /dev/mst/mt4119_pciconf0 set SRIOV_EN=1 NUM_OF_VFS=120
</code></pre>
<p>Choose 'yes' when asked whether to apply the configuration.</p>
</li>
<li>
<p>Reboot the machine.</p>
</li>
<li>
<p>Verify that the modification worked correctly.</p>
<p>Run:</p>
<pre><code>sudo mst start
sudo mst status
mlxconfig -d &lt;pci_device_path&gt; q | egrep 'NUM_OF_VFS|SRIOV_EN'
</code></pre>
<p>Ensure that the output of the last command matches the changes you have made prior to rebooting.</p>
</li>
</ol>
<h2 id="rdma_hardware_daemon_set">RDMA Hardware Daemon Set<a class="headerlink" href="#rdma_hardware_daemon_set" title="Permanent link">&para;</a></h2>
<p>This section covers the installation of the RDMA Hardware Daemon Set onto all of the worker nodes in the Kubernetes cluster.</p>
<ol>
<li>
<p>Use Kubernetes to deploy the Daemon Set to all the nodes in the cluster.</p>
<p>On the master node of the Kubernetes cluster, run:</p>
<pre><code>kubectl apply -f &lt;rdma_daemonset_yaml&gt;
</code></pre>
<p>Where &lt;rdma_daemonset_yaml> is a YAML file that specifies the details of the Daemon Set. This file can be found at:</p>
<pre><code>https://github.com/rit-k8s-rdma/rit-k8s-rdma-ds/blob/master/rdma-ds.yaml
</code></pre>
<p>Applying this Daemon Set configuration on the Kubernetes cluster will cause each worker node to pull container images built as part of this project from our <a href="https://hub.docker.com/u/ritk8srdma">Docker Hub repository</a>.</p>
<p>If you would like to build this Docker image yourself, the instructions are available within the <a href="https://github.com/rit-k8s-rdma/rit-k8s-rdma-ds">RDMA Hardware Daemon Set repository</a>.</p>
</li>
<li>
<p>Verify that the RDMA Hardware Daemon Set is running on each worker node in the cluster.</p>
<p>On the master node of the Kubernetes cluster, run:</p>
<pre><code>kubectl get pods -o wide --namespace kube-system
</code></pre>
<p>Within the output for this command, you should see several lines with the name: <code>rdma-ds-*</code> (one for each worker node in the cluster). The <code>status</code> column of each of these pods will show <code>Init:0/1</code> while the pod is performing hardware enumeration and initialization of the SR-IOV enabled RDMA hardware. One this has completed (it may take several minutes if there are a large number of virtual functions configured on a host), the status of the pods should switch to <code>Running</code>.</p>
</li>
<li>
<p>Verify that the RDMA Hardware Daemon Set's REST endpoint is running.</p>
<p>Once the status of each hardware daemon set pod, as inspected in the previous step, is <code>Running</code>, the REST endpoint should be available to query. During the normal operation of the system, this would be done by the scheduler extension during the scheduling of a new pod. However, we can perform this process manually using 'curl' or a web browser.</p>
<p>First, determine the IP address or hostname of the worker node whose RDMA Hardware Daemon Set you would like to test. Then, in a web browser, navigate to:</p>
<pre><code>http://&lt;IP_OR_HOSTNAME&gt;:54005/getpfs
</code></pre>
<p>Where &lt;IP_OR_HOSTNAME> is the IP or hostname of the worker node whose daemon set you would like to test, and 54005 is the port that daemon set is listening on (54005 is the default value at the time of writing).</p>
<p>You should see output that resembles the following:</p>
<pre><code>[
    {
        "name":"enp4s0f0",
        "used_tx_rate":0,
        "capacity_tx_rate":100000,
        "used_vfs":0,
        "capacity_vfs":120,
        "vfs":[
            {"vf":0, "mac":"7a:90:db:7b:30:ac", "vlan":0, "qos":0, "vlan_proto": "N/A", "spoof_check":"OFF", "trust":"ON", "link_state":"Follow", "min_tx_rate":0, "max_tx_rate":0, "vgt_plus":"OFF", "rate_group":0, "allocated":false},
            {"vf":1, "mac":"c6:26:fe:e2:4e:95", "vlan":0, "qos":0, "vlan_proto":"N/A", "spoof_check":"OFF", "trust":"ON", "link_state":"Follow", "min_tx_rate":0, "max_tx_rate":0, "vgt_plus":"OFF", "rate_group":0, "allocated":false},
            ...
        ]
    },
    {
        "name":"enp4s0f1",
        "used_tx_rate":0,
        "capacity_tx_rate":100000,
        "used_vfs":0,
        "capacity_vfs":120,
        "vfs":[
            {"vf":0, "mac":"02:b9:9a:99:9e:ac", "vlan":0, "qos":0, "vlan_proto":"N/A", "spoof_check":"OFF", "trust":"ON", "link_state":"Follow", "min_tx_rate":0, "max_tx_rate":0, "vgt_plus":"OFF", "rate_group":0, "allocated":false},
            {"vf":1, "mac":"ea:99:de:00:e8:8b", "vlan":0,"qos":0, "vlan_proto":"N/A", "spoof_check":"OFF", "trust":"ON", "link_state":"Follow", "min_tx_rate":0, "max_tx_rate":0, "vgt_plus":"OFF", "rate_group":0, "allocated":false},
            ...
        ]
    },
    ...
]
</code></pre>
<p>If the RDMA Hardware Daemon Set endpoint responds, and the hardware information presented in the list it returns accurately reflects the state and details of the RDMA hardware on its node, then it is working correctly.</p>
<p>Respeat this process for every node in the cluster to verify that each instance of the daemon set is working correctly.</p>
</li>
</ol>
<h2 id="scheduler_extension">Scheduler Extension<a class="headerlink" href="#scheduler_extension" title="Permanent link">&para;</a></h2>
<p>This section covers the installation of the Scheduler Extension component.</p>
<ol>
<li>
<p>Install and run the scheduler extension Docker container on the master node of the Kubernetes cluster.</p>
<p>Run</p>
<pre><code>docker run -d --rm --name ritk8srdma-scheduler-entension --network host ritk8srdma/rit-k8s-rdma-scheduler-extender
</code></pre>
<p>This will pull the Docker image for the scheduler extension from our Docker Hub repository and run it.</p>
<p>If you would like to build the scheduler extension docker image yourself, the instructions are available within the <a href="https://github.com/rit-k8s-rdma/rit-k8s-rdma-scheduler-extender">scheduler extension repository</a>.</p>
</li>
<li>
<p>Modify the configuration of the core Kubernetes scheduler to register the scheduler extension.</p>
<p>On the master node of the Kubernetes cluster, edit or add the file /etc/kubernetes/scheduler-policy-config.json to register the scheduler extension. The following entry should be added to the 'extenders' list within that file:</p>
<pre><code>{
    "urlPrefix": "http://127.0.0.1:8888/scheduler",
    "filterVerb": "rdma_scheduling",
    "bindVerb": "",
    "enableHttps": false,
    "nodeCacheCapable": false,
    "ignorable": false
}
</code></pre>
<p>Here, the IP address/port combination of 127.0.0.1 and 8888 is used because the scheduler extension is running on the same node as the core Kubernetes scheduler (the master node of the Kubernetes cluster), and listening on port 8888. If the extension is run elsewhere or listening on a different port, the 'urlPrefix' parameter should be editted accordingly.</p>
<p>An example version of this file is available in the <a href="https://github.com/rit-k8s-rdma/rit-k8s-rdma-scheduler-extender/blob/master/kube_scheduler_config_files/scheduler-policy-config.json">scheduler extension repository</a>.</p>
</li>
<li>
<p>Ensure the core Kubernetes scheduler is using the configuration file where the scheduler extension is registered.</p>
<p>Edit the file /etc/kubernetes/manifests/kube-scheduler.yaml on the master node of the Kubernetes cluster. Add the following volume to the pod definition if it does not exist (place the definition within the existing 'volumes' section if one exists):</p>
<pre><code>volumes:
- hostPath:
    path: /etc/kubernetes/scheduler-policy-config.json
    type: FileOrCreate
  name: scheduler-policy-config
</code></pre>
<p>Add the following directive to the command to be run in the kube-scheduler container:</p>
<pre><code>--policy-config-file=/etc/kubernetes/scheduler-policy-config.json
</code></pre>
<p>Overall, the whole file should look like:</p>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  annotations:
    scheduler.alpha.kubernetes.io/critical-pod: ""
  creationTimestamp: null
  labels:
    component: kube-scheduler
    tier: control-plane
  name: kube-scheduler
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-scheduler
    - --address=127.0.0.1
    - --kubeconfig=/etc/kubernetes/scheduler.conf
    - --policy-config-file=/etc/kubernetes/scheduler-policy-config.json
    - --leader-elect=true
    image: k8s.gcr.io/kube-scheduler:v1.13.5
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 8
      httpGet:
        host: 127.0.0.1
        path: /healthz
        port: 10251
        scheme: HTTP
      initialDelaySeconds: 15
      timeoutSeconds: 15
    name: kube-scheduler
    resources:
      requests:
        cpu: 100m
    volumeMounts:
    - mountPath: /etc/kubernetes/scheduler.conf
      name: kubeconfig
      readOnly: true
    - mountPath: /etc/kubernetes/scheduler-policy-config.json
      name: scheduler-policy-config
      readOnly: true
  hostNetwork: true
  priorityClassName: system-cluster-critical
  volumes:
  - hostPath:
      path: /etc/kubernetes/scheduler.conf
      type: FileOrCreate
    name: kubeconfig
  - hostPath:
      path: /etc/kubernetes/scheduler-policy-config.json
      type: FileOrCreate
    name: scheduler-policy-config
status: {}
</code></pre>
</li>
<li>
<p>Ensure that the scheduler extension has started up correctly.</p>
<p>Run <code>docker logs ritk8srdma-scheduler-entension</code> on the node where the scheduler extension is running. The output should include the following line at the top of its output:</p>
<pre><code>YYYY/MM/DD HH:MM:SS RDMA scheduler extender listening on port:  &lt;port_number&gt;
</code></pre>
<p>This command can be run whenever necessary to view the logging output from the scheduler extension.</p>
</li>
</ol>
<h2 id="cni_plugin">CNI Plugin<a class="headerlink" href="#cni_plugin" title="Permanent link">&para;</a></h2>
<p>This section covers the installation of the CNI plugin on each RDMA-enabled worker node in the Kubernetes cluster.</p>
<ol>
<li>
<p>Install the CNI plugin executable.</p>
<p>Copy the 'rit-k8s-rdma-cni-linux-amd64' executable from <a href="https://github.com/rit-k8s-rdma/rit-k8s-rdma-sriov-cni/releases">the releases page of the repository</a>. Place it in /opt/cni/bin/ on each RDMA-enabled worker node in the Kubernetes cluster.</p>
</li>
<li>
<p>Install the CNI plugin configuration file.</p>
<p>Copy the file '10-ritk8srdma-cni.conf' from <a href="https://github.com/rit-k8s-rdma/rit-k8s-rdma-sriov-cni/releases">the releases page of the repository</a>. Place it in /etc/cni/net.d/ on each RDMA-enabled worker node in the Kubernetes cluster. This configuration file can be edited to fit the needs of your environment. Ensure that this file is the first one (lexicographically) within that directory. Kubernetes always uses the CNI configuration that comes first lexicographically within this directory.</p>
<p>Within this file, the 'type' parameter specifies the name of the CNI executable that will be run when a pod is deployed. This name should match the name of the executable installed during step 1.</p>
</li>
</ol>
<p>To compile the CNI plugin binary yourself, checkout the <a href="https://github.com/rit-k8s-rdma/rit-k8s-rdma-sriov-cni">CNI repository</a>. Enter the 'sriov' directory within the checked-out copy of the repository, then run <code>go install</code>. The binary should then be created in the appropriate Golang bin directory.</p>
<h2 id="dummy_device_plugin">Dummy Device Plugin<a class="headerlink" href="#dummy_device_plugin" title="Permanent link">&para;</a></h2>
<p>This section covers the installation of the Dummy Device Plugin onto all of the worker nodes in the Kubernetes cluster.</p>
<ol>
<li>
<p>Use Kubernetes to deploy the dummy device plugin to all the nodes in the cluster.</p>
<p>On the master node of the Kubernetes cluster, run:</p>
<pre><code>kubectl apply -f &lt;dummy_plugin_yaml&gt;
</code></pre>
<p>Where &lt;dummy_plugin_yaml> is a YAML file that specifies the details of the Dummy Device Plugin. This file can be found at:</p>
<pre><code>https://github.com/rit-k8s-rdma/rit-k8s-rdma-dummy-device-plugin/blob/master/rdma-dummy-dp-ds.yaml
</code></pre>
<p>Applying this configuration on the Kubernetes cluster will cause each worker node to pull container images built as part of this project from our <a href="https://hub.docker.com/u/ritk8srdma">Docker Hub repository</a>. These images contain the files necessary to run the Dummy Device Plugin.</p>
<p>If you would like to build the Dummy Device Plugin Docker image yourself, the instructions are available within the <a href="https://github.com/rit-k8s-rdma/rit-k8s-rdma-dummy-device-plugin">Dummy Device Plugin repository</a>.</p>
</li>
<li>
<p>Verify that the Dummy Device Plugin is running on each worker node in the cluster.</p>
<p>On the master node of the Kubernetes cluster, run:</p>
<pre><code>kubectl get pods -o wide --namespace kube-system
</code></pre>
<p>Within the output for this command, you should see several lines with the name: <code>rdma-dummy-dp-ds-*</code> (one for each worker node in the cluster). The <code>status</code> column of each of these pods should show <code>Running</code>.</p>
</li>
</ol>
<h2 id="pod_yaml_changes">Pod YAML Changes<a class="headerlink" href="#pod_yaml_changes" title="Permanent link">&para;</a></h2>
<pre><code>To take advantage of RDMA within a Kubernetes pod, that pod's definition (YAML file) will need to be updated to specify the RDMA interfaces that it requires. This involves the following steps:
</code></pre>
<ol>
<li>
<p>Add the <code>rdma_interfaces_required</code> directive to the pod's metadata annotations:</p>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: pod1
  annotations:
    rdma_interfaces_required: '[
        {"min_tx_rate": 15000, "max_tx_rate": 20000},
        {"min_tx_rate": 5000},
        {}
    ]'
spec:
...
</code></pre>
<p>The value of this annotation should be a JSON-formatted string that contains a list of RDMA interfaces needed by the pod, as well as the bandwidth limitations and reservations for each of those interfaces. In this case <code>min_tx_rate</code> specifies an amount of bandwidth that should be reserved for the pod to use exclusively through a specific RDMA interface, while <code>max_tx_rate</code> sets a cap on the amount of bandwidth that can used by a pod through an interface. Either or both of these properties can be omitted if you do not need a bandwidth cap/reservation. In the example above, three RDMA interfaces are requested by a pod: the first sets both properties, the second has only a bandwidth reservation, and the third has no limit nor any reserved bandwidth. The numbers used are in units of megabits of bandwidth per second (Mb/S).</p>
</li>
<li>
<p>Add a request for a <code>/dev/infiniband/</code> mount to each container that will need access to RDMA interfaces:</p>
<pre><code>...
  containers:
  - image: mellanoxubuntudocker:latest
    name: mofed-test-ctr1
    resources:
      limits:
        rdma-sriov/dev-infiniband-mount: 1
...
</code></pre>
<p>The line <code>rdma-sriov/dev-infiniband-mount: 1</code> indicates that the container requires privleged access to the <code>/dev/infiniband</code> directory. The quantity specified should be 1 (technically, the dummy device plugin is advertising an infinite amount of this resource type, but only one is needed to provide the mount point to the container).</p>
</li>
<li>
<p>Make sure the container images being deployed in the pod contain the necessary RDMA libraries. This can be done by utilizing Mellanox's 'mellanoxubuntudocker:latest' Docker container image (and/or using this as a base to build other containers).</p>
</li>
</ol>
<p>An example of a complete pod configuration file is available from our <a href="https://github.com/rit-k8s-rdma/rit-k8s-rdma-common/blob/master/example-pod-config.yml">common repository</a>.</p>
<h2 id="testing_and_verification">Testing and Verification<a class="headerlink" href="#testing_and_verification" title="Permanent link">&para;</a></h2>
<p>In order to verify that the system is working correctly, we can perform several tests. These tests will involve performing actions that exercise some components of the system, then performing checks to ensure those components functioned correctly.</p>
<h3 id="deploy_a_pod">Deploy a Pod<a class="headerlink" href="#deploy_a_pod" title="Permanent link">&para;</a></h3>
<p>In this test, we deploy a single pod within the cluster to ensure that basic connectivity exists between the scheduler extension, dummy device plugin, and CNI plugin.</p>
<ol>
<li>
<p>Create a YAML file describing a pod to be deployed. Outfit this YAML file with the annotations mentioned in the 'Pod YAML Changes' section.</p>
</li>
<li>
<p>Request that Kubernetes deploy the pod onto the cluster. Running this command will indicate to Kubernetes that you would like to run the pod on one of the worker nodes in the cluster.</p>
<pre><code>kubectl create -f &lt;pod_yaml_filename&gt;
</code></pre>
</li>
<li>
<p>View the status of the pod. Running this command will inform you where in the deployment process the pod is currently.</p>
<pre><code>kubectl get pods -o wide
</code></pre>
<p>Find the entry in the list that matches the name of the pod in your YAML file from step 1. You want the pod's status to be <code>Running</code>, though it may take several seconds to reach this state even when everything is working correctly, depending on the size of your cluster and the pod's resource requirements.</p>
<p>If the pod does not reach the <code>Running</code> state after waiting for a while, you will need to perform some troubleshooting. First, observe more detailed information about the status of the pod by running:</p>
<pre><code>kubectl describe pod &lt;POD_NAME&gt;
</code></pre>
<p>Where &lt;POD_NAME> is the name of the pod in your YAML file from step 1. Included in the information retrieved by this command is a the annotations used to request RDMA interface(s) for the pod, as well as the state of each container within the pod. When containers have a status other than <code>Running</code>, they will include a reason, such as <code>ErrImagePull</code>, which means the Docker image used to create the container couldn't be found on the node the pod was deployed on, or on Docker Hub. Also included in the output from this command is a list of 'Events' that have occured for the pod. If this events log contains a message like:</p>
<pre><code>Post http://127.0.0.1:8888/scheduler/rdma_scheduling: dial tcp 127.0.0.1:8888: connect: connection refused
</code></pre>
<p>Then the scheduler extension could not be contacted by the core Kubernetes scheduler. Make sure the scheduler is running, and is configured to listen on the port the scheduler extension is trying to reach.</p>
<p>Alternatively, if a message like the following appears in the event log:</p>
<pre><code>&lt;N&gt; RDMA Scheduler Extension: Unable to collect information on available RDMA resources for node.
</code></pre>
<p>Then the scheduler extension experienced a timeout or a failed connection when attempting to contact the RDMA Hardware Daemon Set instanced on &lt;N> of the nodes in the cluster. If this occurs, repeat the verification steps from the RDMA Hardware Daemon Set installation section. If this works, then verify that the port and URL being requested from within the scheduler extension match those that the daemon set is listening on. It may be helpful to look at the scheduler extension logs in this case.</p>
<p>Another possible message is the following:</p>
<pre><code>&lt;N&gt; RDMA Scheduler Extension: Node did not have enough free RDMA resources.
</code></pre>
<p>Which means that the scheduler extension received information that indicated the available RDMA VFs/bandwidth on &lt;N> of the nodes in the cluster was not enough to satisfy the pod's requirements. If this seems incorrect, repeat step 3 of the RDMA Hardware Daemon Set install section, and inspect the amount of PFs, Bandwidth, and VFs available are correct for the node (and can satisfy what the pod is requesting).</p>
<p>Yet another message that may show up in the events log for a pod is the following:</p>
<pre><code>&lt;N&gt; RDMA Scheduler Extension: 'rdma_interfaces_required' field in pod YAML file is malformatted.
</code></pre>
<p>This simply means that the value in the <code>rdma_interfaces_required</code> annotation for the pod is not a valid JSON string. Simply delete the pending pod, fix the error, and re-deploy. It may be helpful to look at the scheduler extension logs in this case as well.</p>
<p>An error like:</p>
<pre><code>&lt;N&gt; Insufficient rdma-sriov/dev-infiniband-mount.
</code></pre>
<p>Comes from not having the dummy device plugin installed and configured correctly. View the status of its pods with <code>kubectl get pods -o wide --namespace kube-system</code>, and view the log files of each instance by executing <code>docker logs</code> on the right container on each worker node.</p>
<p>Finally, a few other errors specific to our system may come from the CNI plugin. These will be marked as such, but are also unexpected if the CNI plugin is installed at all. In order to view the logs for a specific instance of our CNI plugin that appears to have an issue, search the relevant node's system log for messages that begin with <code>RIT-CNI</code>.</p>
</li>
</ol>
<h3 id="test_connectivity_between_two_pods">Test connectivity between two pods<a class="headerlink" href="#test_connectivity_between_two_pods" title="Permanent link">&para;</a></h3>
<p>In order to ensure that the system has correctly provisioned pods with RDMA interfaces and set the correct bandwidth limits on those interfaces, we can run two pods on separate nodes in the Kubernetes cluster, then perform a bandwidth test between them.</p>
<ol>
<li>
<p>Create two pod YAML files with the necessary information to request at least one RDMA interface. Add node selectors for this test to ensure that the two pods are deployed to two different nodes within the cluster (naturally, the nodes chosen should have enough RDMA resources available to satisfy the rquirements of the pods, the scheduler extension will prevent the pods from being deployed otherwise). Also, ensure the container images used for these pods contain the 'ib_send_bw' RDMA testing utility, as well as the necessary RDMA libraries.</p>
</li>
<li>
<p>Deploy the two pods onto their respective nodes in the cluster. Follow the instructions from the 'Deploy a Pod' section for troubleshooting.</p>
</li>
<li>
<p>On the node to which the first pod has been deployed, find the Docker container running within that pod using <code>docker ps</code>. Execute a shell within that container by running:</p>
<pre><code>docker exec -ti &lt;CONTAINER_ID&gt; /bin/bash
</code></pre>
<p>Where &lt;CONTAINER_ID> is the ID of the container that you found by running <code>docker ps</code>.</p>
</li>
<li>
<p>While in a shell within that container, perform the following actions:</p>
<p>Get the pods IP address (use the IP for the eth0 interface):</p>
<pre><code>ifconfig
</code></pre>
<p>Get the name of the RDMA adapter for the VF that was allocated to the pod's eth0 interface:</p>
<pre><code>ibdev2netdev
</code></pre>
<p>Run the receiving (server) side of the bandwidth testing application:</p>
<pre><code>ib_send_bw -d &lt;RDMA_ADAPTER_NAME&gt; -i 1 -F --report_gbits --run_infinitely
</code></pre>
<p>Where &lt;RDMA_ADAPTER_NAME> is the name of the form 'mlx5_N' which was connected to the interface whose IP address you found when running <code>ifconfig</code>.</p>
<p>This application is now waiting for an incoming connection. We will run the sending/client end of the test from a container within the other pod that we deployed onto another node in the cluster.</p>
</li>
<li>
<p>On the node to which the second pod has been deployed, find a Docker container running within that pod and execute a shell within it (follow the same process as you did for the first pod).</p>
</li>
<li>
<p>Within the shell running in the container from the second pod, perform the following actions:</p>
<p>Get the name of the RDMA adapter for the VF that was allocated to the pod's eth0 interface:</p>
<pre><code>ibdev2netdev
</code></pre>
<p>Run the sending (client) side of the bandwidth testing application:</p>
<pre><code>ib_send_bw -d &lt;RDMA_ADAPTER_NAME&gt; -i 1 -F --report_gbits &lt;SERVER_IP&gt; --run_infinitely
</code></pre>
<p>Where &lt;RDMA_ADAPTER_NAME> is the name of the form 'mlx5_N' which was connected to the eth0 interface, and &lt;SERVER_IP> is the IP address you found for the interface within the other (first) pod in step 4.</p>
<p>This command should display a table of bandwidth measurements that is updated periodically as it continues to run. The measurements displayed in the <code>BW average[Gb/sec]</code> column should all be at or below the bandwidth limit you set on the sending pod within its YAML file. If this is the case, then bandwidth limitation is working. A similar test can be run for bandwidth reservation, using additional pairs of pods that compete for bandwidth with the one that has the reservation.</p>
</li>
</ol>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../introduction/" class="btn btn-neutral float-right" title="Introduction">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../helpAndTools/" class="btn btn-neutral" title="helpAndTools"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
	  
        </div>
      </div>

    </section>

  </div>

<div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../helpAndTools/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../introduction/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>

</body>
</html>
