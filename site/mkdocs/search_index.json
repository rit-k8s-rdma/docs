{
    "docs": [
        {
            "location": "/",
            "text": "Welcome to \nrit-k8s-rdma\n docs\n\u00b6\n\n\nThis project aims to develop software components that allows the use of RDMA\nfrom within containers managed by Kubernetes. It differs from the previous\nprojects it builds off of by allowing for more fine grained control over\nvarious aspects of the system, including the ability to set bandwidth limits\nand reserve bandwidth for specific Kubernetes pods.\n\n\nIntroduction\n\u00b6\n\n\nFor a overview of the system, its design, and the problem it solves, see the \nintroduction page\n.\n\n\nComponents\n\u00b6\n\n\nFor a description of each of the software components that are part of the project, see the \ncomponents page\n.\n\n\nInstall\n\u00b6\n\n\nFor an installation guide, see the \ninstall page\n.\n\n\nHelpful Commands and Tools\n\u00b6\n\n\nA list of useful tools and commands outside of this project is available on the \nhelpful commands and tools page\n.\n\n\nGlossary\n\u00b6\n\n\nFor definitions of terms and acronyms, see the \nglossary page\n.",
            "title": "Home"
        },
        {
            "location": "/#welcome_to_rit-k8s-rdma_docs",
            "text": "This project aims to develop software components that allows the use of RDMA\nfrom within containers managed by Kubernetes. It differs from the previous\nprojects it builds off of by allowing for more fine grained control over\nvarious aspects of the system, including the ability to set bandwidth limits\nand reserve bandwidth for specific Kubernetes pods.",
            "title": "Welcome to rit-k8s-rdma docs"
        },
        {
            "location": "/#introduction",
            "text": "For a overview of the system, its design, and the problem it solves, see the  introduction page .",
            "title": "Introduction"
        },
        {
            "location": "/#components",
            "text": "For a description of each of the software components that are part of the project, see the  components page .",
            "title": "Components"
        },
        {
            "location": "/#install",
            "text": "For an installation guide, see the  install page .",
            "title": "Install"
        },
        {
            "location": "/#helpful_commands_and_tools",
            "text": "A list of useful tools and commands outside of this project is available on the  helpful commands and tools page .",
            "title": "Helpful Commands and Tools"
        },
        {
            "location": "/#glossary",
            "text": "For definitions of terms and acronyms, see the  glossary page .",
            "title": "Glossary"
        },
        {
            "location": "/components/",
            "text": "Components\n\u00b6\n\n\nThe following are a detailed description of each of the components that make up the solution.\n\n\nRDMA Hardware Daemon Set\n\u00b6\n\n\nThe RDMA Harware Daemon Set is a \nKubernetes Daemon Set\n that has two tasks. The first task is to initialize the RDMA SRIOV enabled interfaces on a node and the second task is to create a RESTful endpoint for providing metadata about the PF's and their associated VF's that exist on a given node that are part of the Kubernetes cluster. Further detail about each of the containers within the RDMA Hardware Daemon set can be seen below:\n\n\n\n\nInit Container - the init container is a privileged container that runs as a \nKubernetes Init Container\n, in simplified terms this means it is run before any other container within the pod. The reason that the container needs to be run as a privileged container is because it will be working on a nodes network devices, which means it needs special access in order to configure them. The init container does two things, the first is to scan all the available interfaces on the node and determine if it is an RDMA device and whether SRIOV is enabled. For all interfaces that meet those two requirements, the init container configures each interfaces VF's to be available and ready to run.\n\n\nServer Container - the server container is an unprivileged container that scans through all available interfaces when starting up. It then makes a list of all interfaces that are SRIOV enabled and upon a RESTful get request from a user will return associated metadata about the container. The server container only scans the interfaces at startup because SRIOV devices can only change configurations upon a machine restart, which would rerun the init container and then the server container. The RESTful endpoint serves data in a JSON formatted list of PF's and each PF has an internal list of VF's, more info can be found \nhere\n. The RESTful endpoint that the server container sets up is bound to the host network.\n\n\n\n\nBoth the init container and the server container run under the same RDMA Hardware Daemon Set pod. When configuring how to install this pod please look at \ndaemon set install instructions\n for more detail.\n\n\nScheduler Extension\n\u00b6\n\n\nThe scheduler extension is a software component that is responsible for ensuring that RDMA-enabled pods are deployed onto nodes with enough RDMA resources (bandwidth and virtual functions) to support them. The scheduler extension runs alongside the kube-scheduler process within a Kubernetes cluster, and listens for HTTP requests on a TCP port (8888 by default). The extension is then registered within the configuration file for kube-scheduler (see the \ninstall page\n for details). Every time a new pod is being scheduled, kube-scheduler will make an HTTP request to the scheduler extenstion that contains the details of the new pod as well as a list of nodes within the cluster that kube-scheduler believe to be eligible to deploy the pod onto. The scheduler extension's job is to filter down that list based on whether the nodes in it have enough free RDMA VFs and bandwidth to support the pods requirements or not.\n\n\nIn order to accomplish this, the scheduler extension contacts the RDMA hardware daemon set running on each of the potential nodes in the cluster to gather information about the current allocation of RDMA resources on that node. The requests to each daemon set are made in parallel, with a timeout for cases where a node's daemon set doesn't respond. Once the information has been gathered from a node, the scheduler extension calculates whether or not the node's interface and reserved bandwidth requirements can be met by the remaining resources on that node. If they can be, the node is added to a list of those that are elligible to deploy the pod onto. Once all of the nodes in the list passed in by kube-scheduler have been contacted or have timed out, this list is returned to kube-scheduler in an HTTP response. From here, kube-scheduler will limit its choice of where to place the new pod to one of the nodes in the returned list. If the returned list is empty, the pod will not be scheduled, and the output of \nkubectl describe\n for the pod will show the reasons given by the scheduler extension as to why the nodes in the list that was passed in were not eligible to host the pod.\n\n\nFor help installing the scheduler extension and registering it with kube-scheduler, see the \nscheduler extension\n section of the install page.\n\n\nMore information about the Kubernetes API for scheduler extensions can be found on the Kubernetes \nwebsite\n and \ngithub\n.\n\n\nCNI Plugin\n\u00b6\n\n\nThe container network interface, or CNI, is a specification that governs the way network access is provided to containers and containerized applications. Kubernetes utilizes CNI as its standard for allocating IP addresses and network interfaces to pods that have been deployed in a cluster. A CNI plugin is a piece of software that performs that allocation, along with any other additional setup. Since RDMA network interfaces require specific additional setup to configure bandwidth limits and reservations, as well as to select virtual functions such that a pod's bandwidth requirements are satisfied, a CNI plugin is necessary for handling RDMA-enabled pods.\n\n\nThe CNI plugin developed for this project is a fork of the existing \nMellanox CNI plugin\n, which was limited in the fact that it always allocated one interface to each pod and didn't support bandwidth reservation or limitation. The CNI plugin for this project improves upon this by adding support for an arbitrary number of RDMA interfaces per pod, including the ability to allocate no RDMA interfaces to pods that do not need any.\n\n\nThe CNI plugin is executed each time a pod is created or destroyed. It runs only once per pod for each of these actions, and must allocate or deallocate all of the interfaces for a pod at one time. This is done by executing an algorithm that finds a mapping of requested pod interfaces onto virtual functions that allows a pod's requirements to be satisfied. This is similar to one of the steps performed by the scheduler extension, though the scheduler extension need only determine whether a pod's requirements can be satisfied by a node, rather than what the exact final allocation of VFs to that pod will be to satisfy it.\n\n\nThe tasks performed by the CNI plugin during pod setup are the following:\n\n\n\n\nDetermine a placement of RDMA virtual functions that will satisfy the pod's requirements.\n\n\nMove each of the needed virtual functions into the pod's network namespace.\n\n\nRename each of the virtual functions that have been added to the pod's network namespace.\n\n\nSet the bandwidth limits and reservations on each RDMA VF, if necessary.\n\n\nAllocate IP addresses to each of the VFs that have been allocated to the pod.\n\n\n\n\nThe tasks performed by the CNI plugin during pod teardown are the following:\n\n\n\n\nRename each of the virtual functions in the pod's network namespace.\n\n\nMove each of the virtual functions from the pod's network namespace back to the host's network namespace.\n\n\nRemove any bandwidth reservations or limitations set on the deallocated virtual functions.\n\n\n\n\nDummy Device Plugin\n\u00b6\n\n\nThe Dummy Device Plugin is a stop gap measure for the current system. The directory \n/dev/infiniband\n is needed within any pod that requires RDMA. In order to access devices in this directory a container either needs to be run in privileged mode or a \nDevice Plugin\n can return a directory that a container will have privileged access to. For obvious reasons we opted to create a Device Plugin for the sole purpose that it will give \nany container\n that requests an RDMA device the proper access to \n/dev/infiniband\n. This Dummy Device Plugin does not act like any other Device Plugin where it is meant to manage a specialized resources; that is done by the RDMA Hardware Daemon Set, Scheduler Extender, and CNI components. When configuring how to install this pod please look at \ndummy device plugin installation\n for more detail.",
            "title": "Components"
        },
        {
            "location": "/components/#components",
            "text": "The following are a detailed description of each of the components that make up the solution.",
            "title": "Components"
        },
        {
            "location": "/components/#rdma_hardware_daemon_set",
            "text": "The RDMA Harware Daemon Set is a  Kubernetes Daemon Set  that has two tasks. The first task is to initialize the RDMA SRIOV enabled interfaces on a node and the second task is to create a RESTful endpoint for providing metadata about the PF's and their associated VF's that exist on a given node that are part of the Kubernetes cluster. Further detail about each of the containers within the RDMA Hardware Daemon set can be seen below:   Init Container - the init container is a privileged container that runs as a  Kubernetes Init Container , in simplified terms this means it is run before any other container within the pod. The reason that the container needs to be run as a privileged container is because it will be working on a nodes network devices, which means it needs special access in order to configure them. The init container does two things, the first is to scan all the available interfaces on the node and determine if it is an RDMA device and whether SRIOV is enabled. For all interfaces that meet those two requirements, the init container configures each interfaces VF's to be available and ready to run.  Server Container - the server container is an unprivileged container that scans through all available interfaces when starting up. It then makes a list of all interfaces that are SRIOV enabled and upon a RESTful get request from a user will return associated metadata about the container. The server container only scans the interfaces at startup because SRIOV devices can only change configurations upon a machine restart, which would rerun the init container and then the server container. The RESTful endpoint serves data in a JSON formatted list of PF's and each PF has an internal list of VF's, more info can be found  here . The RESTful endpoint that the server container sets up is bound to the host network.   Both the init container and the server container run under the same RDMA Hardware Daemon Set pod. When configuring how to install this pod please look at  daemon set install instructions  for more detail.",
            "title": "RDMA Hardware Daemon Set"
        },
        {
            "location": "/components/#scheduler_extension",
            "text": "The scheduler extension is a software component that is responsible for ensuring that RDMA-enabled pods are deployed onto nodes with enough RDMA resources (bandwidth and virtual functions) to support them. The scheduler extension runs alongside the kube-scheduler process within a Kubernetes cluster, and listens for HTTP requests on a TCP port (8888 by default). The extension is then registered within the configuration file for kube-scheduler (see the  install page  for details). Every time a new pod is being scheduled, kube-scheduler will make an HTTP request to the scheduler extenstion that contains the details of the new pod as well as a list of nodes within the cluster that kube-scheduler believe to be eligible to deploy the pod onto. The scheduler extension's job is to filter down that list based on whether the nodes in it have enough free RDMA VFs and bandwidth to support the pods requirements or not.  In order to accomplish this, the scheduler extension contacts the RDMA hardware daemon set running on each of the potential nodes in the cluster to gather information about the current allocation of RDMA resources on that node. The requests to each daemon set are made in parallel, with a timeout for cases where a node's daemon set doesn't respond. Once the information has been gathered from a node, the scheduler extension calculates whether or not the node's interface and reserved bandwidth requirements can be met by the remaining resources on that node. If they can be, the node is added to a list of those that are elligible to deploy the pod onto. Once all of the nodes in the list passed in by kube-scheduler have been contacted or have timed out, this list is returned to kube-scheduler in an HTTP response. From here, kube-scheduler will limit its choice of where to place the new pod to one of the nodes in the returned list. If the returned list is empty, the pod will not be scheduled, and the output of  kubectl describe  for the pod will show the reasons given by the scheduler extension as to why the nodes in the list that was passed in were not eligible to host the pod.  For help installing the scheduler extension and registering it with kube-scheduler, see the  scheduler extension  section of the install page.  More information about the Kubernetes API for scheduler extensions can be found on the Kubernetes  website  and  github .",
            "title": "Scheduler Extension"
        },
        {
            "location": "/components/#cni_plugin",
            "text": "The container network interface, or CNI, is a specification that governs the way network access is provided to containers and containerized applications. Kubernetes utilizes CNI as its standard for allocating IP addresses and network interfaces to pods that have been deployed in a cluster. A CNI plugin is a piece of software that performs that allocation, along with any other additional setup. Since RDMA network interfaces require specific additional setup to configure bandwidth limits and reservations, as well as to select virtual functions such that a pod's bandwidth requirements are satisfied, a CNI plugin is necessary for handling RDMA-enabled pods.  The CNI plugin developed for this project is a fork of the existing  Mellanox CNI plugin , which was limited in the fact that it always allocated one interface to each pod and didn't support bandwidth reservation or limitation. The CNI plugin for this project improves upon this by adding support for an arbitrary number of RDMA interfaces per pod, including the ability to allocate no RDMA interfaces to pods that do not need any.  The CNI plugin is executed each time a pod is created or destroyed. It runs only once per pod for each of these actions, and must allocate or deallocate all of the interfaces for a pod at one time. This is done by executing an algorithm that finds a mapping of requested pod interfaces onto virtual functions that allows a pod's requirements to be satisfied. This is similar to one of the steps performed by the scheduler extension, though the scheduler extension need only determine whether a pod's requirements can be satisfied by a node, rather than what the exact final allocation of VFs to that pod will be to satisfy it.  The tasks performed by the CNI plugin during pod setup are the following:   Determine a placement of RDMA virtual functions that will satisfy the pod's requirements.  Move each of the needed virtual functions into the pod's network namespace.  Rename each of the virtual functions that have been added to the pod's network namespace.  Set the bandwidth limits and reservations on each RDMA VF, if necessary.  Allocate IP addresses to each of the VFs that have been allocated to the pod.   The tasks performed by the CNI plugin during pod teardown are the following:   Rename each of the virtual functions in the pod's network namespace.  Move each of the virtual functions from the pod's network namespace back to the host's network namespace.  Remove any bandwidth reservations or limitations set on the deallocated virtual functions.",
            "title": "CNI Plugin"
        },
        {
            "location": "/components/#dummy_device_plugin",
            "text": "The Dummy Device Plugin is a stop gap measure for the current system. The directory  /dev/infiniband  is needed within any pod that requires RDMA. In order to access devices in this directory a container either needs to be run in privileged mode or a  Device Plugin  can return a directory that a container will have privileged access to. For obvious reasons we opted to create a Device Plugin for the sole purpose that it will give  any container  that requests an RDMA device the proper access to  /dev/infiniband . This Dummy Device Plugin does not act like any other Device Plugin where it is meant to manage a specialized resources; that is done by the RDMA Hardware Daemon Set, Scheduler Extender, and CNI components. When configuring how to install this pod please look at  dummy device plugin installation  for more detail.",
            "title": "Dummy Device Plugin"
        },
        {
            "location": "/glossary/",
            "text": "Glossary\n\u00b6\n\n\nBelow is a list of terminology that is good to know in order to better understand the overall application.\n\n\n\n\nRemote Direct Memory Access (RDMA) - is a technology that uses specialized \nnetwork interface cards\n to access the \nRAM\n of a process running in another computer. It circumvents the CPU of both systems allowing it to perform \nzero-copy\n of data from one computer to another. RDMA allows for staggering amounts of speed when transfering data (in the 100Gb/s), without having to involve the CPU for encoding and decoding the information, making it far more efficient than using traditional mechanisms for transfering data.\n\n\nNamespace - a tool used for isolating resources from one another.\n\n\nNetwork Namespace - separates different processes from being able to access the network interfaces and other network related devices from one another.\n\n\nFilesystem Namespace - allows for isolation when it comes to file systems.\n\n\n\n\n\n\nContainer - a namespace that isolates a process from other running on the system.\n\n\nKubernets - an orchestration system for applications that exist within a container such as docker. The orchestration system allows for managing a cluster of nodes running containerized applications.\n\n\nPod - can be one or more containers that share the same network namespace, but have different filesystem namespaces. Must run on at least one node in the cluster.\n\n\nDaemon Set - a pod that is run across all nodes in the cluster.\n\n\n\n\n\n\nSingle Root Input/Output Virtualization (SR-IOV) - is a standard for allowing network interfaces on a system to appear in software more then there physically are. For example, an RDMA card could be virtualized (depending on what the hardward can support) to appear as 127 individual network interfaces in a software system, when in fact there only exists a single network card and interface.\n\n\nVirtual Function (VF) - a representation in software of a physical interface.\n\n\nPhysical Function (PF) - an interface used to manage all VF's.",
            "title": "Glossary"
        },
        {
            "location": "/glossary/#glossary",
            "text": "Below is a list of terminology that is good to know in order to better understand the overall application.   Remote Direct Memory Access (RDMA) - is a technology that uses specialized  network interface cards  to access the  RAM  of a process running in another computer. It circumvents the CPU of both systems allowing it to perform  zero-copy  of data from one computer to another. RDMA allows for staggering amounts of speed when transfering data (in the 100Gb/s), without having to involve the CPU for encoding and decoding the information, making it far more efficient than using traditional mechanisms for transfering data.  Namespace - a tool used for isolating resources from one another.  Network Namespace - separates different processes from being able to access the network interfaces and other network related devices from one another.  Filesystem Namespace - allows for isolation when it comes to file systems.    Container - a namespace that isolates a process from other running on the system.  Kubernets - an orchestration system for applications that exist within a container such as docker. The orchestration system allows for managing a cluster of nodes running containerized applications.  Pod - can be one or more containers that share the same network namespace, but have different filesystem namespaces. Must run on at least one node in the cluster.  Daemon Set - a pod that is run across all nodes in the cluster.    Single Root Input/Output Virtualization (SR-IOV) - is a standard for allowing network interfaces on a system to appear in software more then there physically are. For example, an RDMA card could be virtualized (depending on what the hardward can support) to appear as 127 individual network interfaces in a software system, when in fact there only exists a single network card and interface.  Virtual Function (VF) - a representation in software of a physical interface.  Physical Function (PF) - an interface used to manage all VF's.",
            "title": "Glossary"
        },
        {
            "location": "/helpAndTools/",
            "text": "Helpful Commands and Tools\n\u00b6\n\n\nHere are some helpful commands and some repositories of where to find information that we found helpful.\n\n\nKubernetes Commands\n\u00b6\n\n\nGet nodes:\n\n\nkubectl get nodes\n\n\n\n\nGet pods:\n\n\nkubectl get pods -o wide\n\n\n\n\nGet pods in namespace:\n\n\nkubectl get pods -o wide --namespace=kube-system\n\n\n\n\nDelete all pods, services, and anything else:\n\n\nkubectl delete daemonsets,replicasets,services,deployments,pods,rc --all\n\n\n\n\nTo reset the master node\n\nNOTE\n must reset up all of the clients for kubernetes:\n\n\nsudo kubeadm reset\n\n\n\n\nDelete config map if already exists:\n\n\nkubectl delete configmap <config-map-name> -n kube-system\n\n\n\n\nDelete DameonSet Extension (the DaemeonSet extension is the pod that runs the Mellanox RDMA plugin)\n\n\nkubectl delete ds --namespace kube-system <dameon-set-name>\n\n\n\n\nDeleting a kubernetes pod\n\n\nkubectl delete pod <pod-name>\n\n\n\n\n is the name that shows up when you do \"kubectl get pods\"\n\n\nInterface on RDMA Nodes\n\u00b6\n\n\nAssign IP to a specific interface:\n\n\nsudo ifconfig  <interface-name> <ip>\n\n\n\n\nBring interface up:\n\n\nsudo ifconfig <interface-name> up\n\n\n\n\nGetting VF information:\n\n\n/opt/mellanox/iproute2/sbin/ip link show <interface>\n\n\n\n\nChanging the number VFs\n\u00b6\n\n\nTo change the VFs on RDMA node first run the following command in order to configure the Mellanox NIC:\n\n\nsudo mst start\n\n\n\n\nTo enable SRIOV and change the number of VFs type in:\n\n\nsudo mlxconfig -d /dev/mst/<mellanox-switch> set SRIOV_EN=1 NUM_OF_VFS=120\n\n\n\n\nThen restart the machine with a friendly message:\n\n\nsudo shutdown -r now 'Updating Mellanox Config'\n\n\n\n\nFinding info about Mellanox NIC\n\u00b6\n\n\nStart the Mellanox device:\n\n\nsudo mst start\n\n\n\n\nList information about the driver:\n\n\nsudo mlxconfig -d /dev/mst/mt4119_pciconf0 q\n\n\n\n\nFind the number of Virtual Functions that were created and make sure the SRIOV environment has been enabled:\n\n\nsudo mlxconfig -d /dev/mst/mt4119_pciconf0 q | grep \"NUM_OF_VFS\"\nsudo mlxconfig -d /dev/mst/mt4119_pciconf0 q | grep \"SRIOV_EN\"\n\n\n\n\nTesting Containers on Nodes or Containers\n\u00b6\n\n\nIf you want to run this on a container, go to the node that the pod is running on and run \ndocker ps\n, find the Container ID of the pod you launched (NOT the pause container) and run \ndocker exec -it <container_id> /bin/bash\n. After that complete the instructions below.\n\n\nAfter you are inside a container or on a system that has a mellanox card running the the following:\n\n\nifconfig -a\n\n\n\n\nThis will list all the interfaces available.\n\n\nThen run:\n\n\nibdev2netdev\n\n\n\n\nThis will give you a list of adapters that you will need in order to connect them to interfaces on the actual system.\n\n\nOne container will be the server and the other will be the client:\n - Server: \nib_send_bw -d <rdma_adapter_name> -i 1 -F --report_gbits --run_infinitely\n\n   - \n is taken from running \"ibdev2netdev -v\"\n     - ex: mlx5_2\n   - Command Ex: \nib_send_bw -d mlx5_2 -i 1 -F --report_gbits --run_infinitely\n\n - Client: \nib_send_bw -d <rdma_adapter_name> -i 1 -F --report_gbits <server_ip> --run_infinitely\n\n   - \n is the IP of the pod that the 'server' testing command was run on\n   - Command Ex: \nib_send_bw -d mlx5_5 -i 1 -F --report_gbits 10.55.206.84 --run_infinitely\n\n\nErrors\n\u00b6\n\n\nCommon errors\n\n\nKubernetes Failing to startup\n\u00b6\n\n\nIf you receive something similar to the following error:\n\n\nThe connection to the server 129.21.34.14:6443 was refused - did you specify the right host or port?\n\n\n\n\nMost likely it is because the process \nkubelet\n failed to start. For some reason it requires the swap space to be off.\nRun the following command to turn it up and after it the kubelet process should start running.\n\n\nswapoff -a\n\n\n\n\nUnable to Open File Descriptor\n\u00b6\n\n\nThis has something to do with how the VFs are allocated and changed (AKA we are not entirely sure, but you should follow this guide or it will fail to latch to sockets). Here is the example error:\n\n\nCouldn't connect to 10.55.206.82:18515\nUnable to open file descriptor for socket connection Unable to init the socket connection\n\n\n\n\nTo remdy this error as well as correctly change number of VFs do the following.\n\n\nOn Skya (the kubelet master) run:\n\n\nkubectl delete pod <pods>\n\n\n\n\nRepositories and Guides\n\u00b6\n\n\nLots of information of repos and information that is helpful:\n\n\nMellanox Rate Limiting\n\u00b6\n\n\nThe commands are taken from https://community.mellanox.com/s/article/kubernetes-ipoib-ethernet-rdma-sr-iov-networking-with-connectx4-connectx5\n\nNOTE\n throughout the notes mt4119_pciconf0 is used, in reality run \nls /dev/mst/<name>\n to find the name of your device\n\n\n\n\n\"how to configure rate limit per VF\": \nhttps://community.mellanox.com/s/article/howto-configure-rate-limit-per-vf-for-connectx-4-connectx-5\n\n\n\"how to set virtual network attributes on a VF\": \nhttps://community.mellanox.com/s/article/howto-set-virtual-network-attributes-on-a-virtual-function--sr-iov-x\n\n\n\n\nRepos\n\u00b6\n\n\nMellanox\n\u00b6\n\n\n\n\nPhysical: \nhttps://github.com/Mellanox/k8s-rdma-sriov-dev-plugin\n\n\nHCA-only: \nhttps://community.mellanox.com/s/article/kubernetes-rdma--infiniband--shared-hca-with-connectx4-connectx5\n\n\nSR-IOV: \nhttps://github.com/Mellanox/sriov-cni\n\n\nInfo: \nhttps://blog.scottlowe.org/2009/12/02/what-is-sr-iov/\n\n\n\n\n\n\nDockerFiles: \nhttps://github.com/Mellanox/mofed_dockerfiles\n\n\n\n\nContainer Networking Interface\n\u00b6\n\n\n\n\nhttps://github.com/containernetworking/cni\n\n\nSpecification:\n\n\nhttps://github.com/containernetworking/cni/blob/master/SPEC.md\n\n\n\n\n\n\n\n\nLinux Kernel:\n\u00b6\n\n\n\n\nhttps://github.com/torvalds/linux\n\n\nhttps://github.com/torvalds/linux/tree/master/drivers/infiniband/core\n\n\nhttps://github.com/torvalds/linux/tree/master/drivers/net/ethernet/mellanox/mlx5/core\n\n\n\n\n\n\n\n\nRDMA Related Information:\n\u00b6\n\n\n\n\n\"Hello World\" type example:\n\n\nhttps://github.com/wangchenghku/rdma_handout\n\n\n\n\n\n\nPerfTest (ib_send_bw, etc.):\n\n\nhttps://github.com/linux-rdma/perftest/tree/master/src\n\n\n\n\n\n\nInformation on the basics of writing an application that uses RDMA:\n\n\nhttps://opensourceforu.com/2016/09/fundamentals-of-rdma-programming/\n\n\n\n\n\n\nRDMA Verbs specification:\n\n\nhttp://www.rdmaconsortium.org/home/draft-hilland-iwarp-verbs-v1.0-RDMAC.pdf\n\n\n\n\n\n\n\"RDMA Core\":\n\n\nMellanox:\n\n\nhttps://github.com/Mellanox/rdma-core\n\n\n\n\n\n\n\"linux-rdma\":\n\n\nhttps://github.com/linux-rdma/rdma-core\n\n\n\n\n\n\n\n\n\n\nManuals:\n\n\nUbuntu Driver Install: \nhttp://www.mellanox.com/related-docs/prod_software/Ubuntu_16_10_Inbox_Driver_User_Manual.pdf\n\n\nRDMA Programming Manuals: \nhttp://www.mellanox.com/related-docs/prod_software/RDMA_Aware_Programming_user_manual.pdf\n\n\nMLNX OFED Manual: \nhttps://docs.mellanox.com/display/MLNXOFEDv451010/MLNX_OFED+v4.5-1.0.1.0+Documentation\n\n\n\n\n\n\nInfo: \nhttp://www.rdmamojo.com/2014/03/31/remote-direct-memory-access-rdma/\n\n\nOther:\n\n\nHustCat CNI RDMA device Plugin: \nhttps://docs.google.com/document/d/1PPOYOdTstnG8-XEHgoCkHEOw5VUOt1blAQNu3MnXuY8/edit\n\n\nMellanox Presentation about RDMA: \nhttps://events.static.linuxfound.org/sites/events/files/slides/containing_rdma_final.pdf\n\n\nMellanox Qos Description: Traffic Classes: \nhttps://community.mellanox.com/s/article/network-considerations-for-global-pause--pfc-and-qos-with-mellanox-switches-and-adapters\n\n\nMellanox Recommended Configuration for deployment: \nhttps://community.mellanox.com/s/article/recommended-network-configuration-examples-for-roce-deployment\n\n\nMOFED and OFED Explanation: \nhttps://www.rohitzambre.com/blog/2018/2/9/for-the-rdma-novice-libfabric-libibverbs-infiniband-ofed-mofed\n\n\nMellanox QoS Infiniband Deployment: http://www.mellanox.com/pdf/whitepapers/deploying_qos_wp_10_19_2005.pdf\n\n\nDescription of what an HCA is: \nhttps://www.ibm.com/support/knowledgecenter/TI0003M/p8ha1/smhostchanneladapter.htm\n\n\nMellanox Driver Description: \nhttps://community.mellanox.com/s/article/mellanox-linux-driver-modules-relationship--mlnx-ofed-x\n\n\nMellanox RDMA-SRIOV Setup: \nhttps://community.mellanox.com/s/article/kubernetes-ipoib-ethernet-rdma-sr-iov-networking-with-connectx4-connectx5\n\n\nMellanox RDMA-HCA Setup: \nhttps://community.mellanox.com/s/article/kubernetes-rdma--infiniband--shared-hca-with-connectx4-connectx5",
            "title": "helpAndTools"
        },
        {
            "location": "/helpAndTools/#helpful_commands_and_tools",
            "text": "Here are some helpful commands and some repositories of where to find information that we found helpful.",
            "title": "Helpful Commands and Tools"
        },
        {
            "location": "/helpAndTools/#kubernetes_commands",
            "text": "Get nodes:  kubectl get nodes  Get pods:  kubectl get pods -o wide  Get pods in namespace:  kubectl get pods -o wide --namespace=kube-system  Delete all pods, services, and anything else:  kubectl delete daemonsets,replicasets,services,deployments,pods,rc --all  To reset the master node NOTE  must reset up all of the clients for kubernetes:  sudo kubeadm reset  Delete config map if already exists:  kubectl delete configmap <config-map-name> -n kube-system  Delete DameonSet Extension (the DaemeonSet extension is the pod that runs the Mellanox RDMA plugin)  kubectl delete ds --namespace kube-system <dameon-set-name>  Deleting a kubernetes pod  kubectl delete pod <pod-name>   is the name that shows up when you do \"kubectl get pods\"",
            "title": "Kubernetes Commands"
        },
        {
            "location": "/helpAndTools/#interface_on_rdma_nodes",
            "text": "Assign IP to a specific interface:  sudo ifconfig  <interface-name> <ip>  Bring interface up:  sudo ifconfig <interface-name> up  Getting VF information:  /opt/mellanox/iproute2/sbin/ip link show <interface>",
            "title": "Interface on RDMA Nodes"
        },
        {
            "location": "/helpAndTools/#changing_the_number_vfs",
            "text": "To change the VFs on RDMA node first run the following command in order to configure the Mellanox NIC:  sudo mst start  To enable SRIOV and change the number of VFs type in:  sudo mlxconfig -d /dev/mst/<mellanox-switch> set SRIOV_EN=1 NUM_OF_VFS=120  Then restart the machine with a friendly message:  sudo shutdown -r now 'Updating Mellanox Config'",
            "title": "Changing the number VFs"
        },
        {
            "location": "/helpAndTools/#finding_info_about_mellanox_nic",
            "text": "Start the Mellanox device:  sudo mst start  List information about the driver:  sudo mlxconfig -d /dev/mst/mt4119_pciconf0 q  Find the number of Virtual Functions that were created and make sure the SRIOV environment has been enabled:  sudo mlxconfig -d /dev/mst/mt4119_pciconf0 q | grep \"NUM_OF_VFS\"\nsudo mlxconfig -d /dev/mst/mt4119_pciconf0 q | grep \"SRIOV_EN\"",
            "title": "Finding info about Mellanox NIC"
        },
        {
            "location": "/helpAndTools/#testing_containers_on_nodes_or_containers",
            "text": "If you want to run this on a container, go to the node that the pod is running on and run  docker ps , find the Container ID of the pod you launched (NOT the pause container) and run  docker exec -it <container_id> /bin/bash . After that complete the instructions below.  After you are inside a container or on a system that has a mellanox card running the the following:  ifconfig -a  This will list all the interfaces available.  Then run:  ibdev2netdev  This will give you a list of adapters that you will need in order to connect them to interfaces on the actual system.  One container will be the server and the other will be the client:\n - Server:  ib_send_bw -d <rdma_adapter_name> -i 1 -F --report_gbits --run_infinitely \n   -   is taken from running \"ibdev2netdev -v\"\n     - ex: mlx5_2\n   - Command Ex:  ib_send_bw -d mlx5_2 -i 1 -F --report_gbits --run_infinitely \n - Client:  ib_send_bw -d <rdma_adapter_name> -i 1 -F --report_gbits <server_ip> --run_infinitely \n   -   is the IP of the pod that the 'server' testing command was run on\n   - Command Ex:  ib_send_bw -d mlx5_5 -i 1 -F --report_gbits 10.55.206.84 --run_infinitely",
            "title": "Testing Containers on Nodes or Containers"
        },
        {
            "location": "/helpAndTools/#errors",
            "text": "Common errors",
            "title": "Errors"
        },
        {
            "location": "/helpAndTools/#kubernetes_failing_to_startup",
            "text": "If you receive something similar to the following error:  The connection to the server 129.21.34.14:6443 was refused - did you specify the right host or port?  Most likely it is because the process  kubelet  failed to start. For some reason it requires the swap space to be off.\nRun the following command to turn it up and after it the kubelet process should start running.  swapoff -a",
            "title": "Kubernetes Failing to startup"
        },
        {
            "location": "/helpAndTools/#unable_to_open_file_descriptor",
            "text": "This has something to do with how the VFs are allocated and changed (AKA we are not entirely sure, but you should follow this guide or it will fail to latch to sockets). Here is the example error:  Couldn't connect to 10.55.206.82:18515\nUnable to open file descriptor for socket connection Unable to init the socket connection  To remdy this error as well as correctly change number of VFs do the following.  On Skya (the kubelet master) run:  kubectl delete pod <pods>",
            "title": "Unable to Open File Descriptor"
        },
        {
            "location": "/helpAndTools/#repositories_and_guides",
            "text": "Lots of information of repos and information that is helpful:",
            "title": "Repositories and Guides"
        },
        {
            "location": "/helpAndTools/#mellanox_rate_limiting",
            "text": "The commands are taken from https://community.mellanox.com/s/article/kubernetes-ipoib-ethernet-rdma-sr-iov-networking-with-connectx4-connectx5 NOTE  throughout the notes mt4119_pciconf0 is used, in reality run  ls /dev/mst/<name>  to find the name of your device   \"how to configure rate limit per VF\":  https://community.mellanox.com/s/article/howto-configure-rate-limit-per-vf-for-connectx-4-connectx-5  \"how to set virtual network attributes on a VF\":  https://community.mellanox.com/s/article/howto-set-virtual-network-attributes-on-a-virtual-function--sr-iov-x",
            "title": "Mellanox Rate Limiting"
        },
        {
            "location": "/helpAndTools/#repos",
            "text": "",
            "title": "Repos"
        },
        {
            "location": "/helpAndTools/#mellanox",
            "text": "Physical:  https://github.com/Mellanox/k8s-rdma-sriov-dev-plugin  HCA-only:  https://community.mellanox.com/s/article/kubernetes-rdma--infiniband--shared-hca-with-connectx4-connectx5  SR-IOV:  https://github.com/Mellanox/sriov-cni  Info:  https://blog.scottlowe.org/2009/12/02/what-is-sr-iov/    DockerFiles:  https://github.com/Mellanox/mofed_dockerfiles",
            "title": "Mellanox"
        },
        {
            "location": "/helpAndTools/#container_networking_interface",
            "text": "https://github.com/containernetworking/cni  Specification:  https://github.com/containernetworking/cni/blob/master/SPEC.md",
            "title": "Container Networking Interface"
        },
        {
            "location": "/helpAndTools/#linux_kernel",
            "text": "https://github.com/torvalds/linux  https://github.com/torvalds/linux/tree/master/drivers/infiniband/core  https://github.com/torvalds/linux/tree/master/drivers/net/ethernet/mellanox/mlx5/core",
            "title": "Linux Kernel:"
        },
        {
            "location": "/helpAndTools/#rdma_related_information",
            "text": "\"Hello World\" type example:  https://github.com/wangchenghku/rdma_handout    PerfTest (ib_send_bw, etc.):  https://github.com/linux-rdma/perftest/tree/master/src    Information on the basics of writing an application that uses RDMA:  https://opensourceforu.com/2016/09/fundamentals-of-rdma-programming/    RDMA Verbs specification:  http://www.rdmaconsortium.org/home/draft-hilland-iwarp-verbs-v1.0-RDMAC.pdf    \"RDMA Core\":  Mellanox:  https://github.com/Mellanox/rdma-core    \"linux-rdma\":  https://github.com/linux-rdma/rdma-core      Manuals:  Ubuntu Driver Install:  http://www.mellanox.com/related-docs/prod_software/Ubuntu_16_10_Inbox_Driver_User_Manual.pdf  RDMA Programming Manuals:  http://www.mellanox.com/related-docs/prod_software/RDMA_Aware_Programming_user_manual.pdf  MLNX OFED Manual:  https://docs.mellanox.com/display/MLNXOFEDv451010/MLNX_OFED+v4.5-1.0.1.0+Documentation    Info:  http://www.rdmamojo.com/2014/03/31/remote-direct-memory-access-rdma/  Other:  HustCat CNI RDMA device Plugin:  https://docs.google.com/document/d/1PPOYOdTstnG8-XEHgoCkHEOw5VUOt1blAQNu3MnXuY8/edit  Mellanox Presentation about RDMA:  https://events.static.linuxfound.org/sites/events/files/slides/containing_rdma_final.pdf  Mellanox Qos Description: Traffic Classes:  https://community.mellanox.com/s/article/network-considerations-for-global-pause--pfc-and-qos-with-mellanox-switches-and-adapters  Mellanox Recommended Configuration for deployment:  https://community.mellanox.com/s/article/recommended-network-configuration-examples-for-roce-deployment  MOFED and OFED Explanation:  https://www.rohitzambre.com/blog/2018/2/9/for-the-rdma-novice-libfabric-libibverbs-infiniband-ofed-mofed  Mellanox QoS Infiniband Deployment: http://www.mellanox.com/pdf/whitepapers/deploying_qos_wp_10_19_2005.pdf  Description of what an HCA is:  https://www.ibm.com/support/knowledgecenter/TI0003M/p8ha1/smhostchanneladapter.htm  Mellanox Driver Description:  https://community.mellanox.com/s/article/mellanox-linux-driver-modules-relationship--mlnx-ofed-x  Mellanox RDMA-SRIOV Setup:  https://community.mellanox.com/s/article/kubernetes-ipoib-ethernet-rdma-sr-iov-networking-with-connectx4-connectx5  Mellanox RDMA-HCA Setup:  https://community.mellanox.com/s/article/kubernetes-rdma--infiniband--shared-hca-with-connectx4-connectx5",
            "title": "RDMA Related Information:"
        },
        {
            "location": "/install/",
            "text": "Install\n\u00b6\n\n\nPrerequisites\n\u00b6\n\n\nBefore installing this system, you should have a working Kubernetes cluster set up. The following prerequisites must be met:\n - Kubernetes version 1.13\n - Golang version 1.12\n   - Version 1.12 or greater is necessary to compile the software components of the system.\n - Mellanox OFED version 4.6-1.0.1.1 or greater\n   - The firmware on Mellanox RDMA cards should be updated to the latest available version.\n\n\nHardware Setup\n\u00b6\n\n\nThis section covers the configuration of Mellanox RDMA hardware in preparation for using SR-IOV.\n\n\n\n\nEnable SR-IOV in the BIOS of an machine with RDMA hardware installed.\n\n\n\n\nLoad the Mellanox driver modules for making configuration changes to the RDMA hardware.\n\n\nRun:\n\n\nsudo mst start\n\n\n\nYou should expect to see output similar to the following:\n\n\nStarting MST (Mellanox Software Tools) driver set\nLoading MST PCI module - Success\nLoading MST PCI configuration module - Success\nCreate devices\nUnloading MST PCI module (unused) - Success\n\n\n\n\n\n\n\nDetermine the path to the PCI device for the RDMA hardware card.\n\n\nRun:\n\n\nsudo mst status\n\n\n\nYou should expect to see output similar to the following:\n\n\nMST modules:\n------------\n    MST PCI module is not loaded\n    MST PCI configuration module loaded\n\nMST devices:\n------------\n/dev/mst/mt4119_pciconf0         - PCI configuration cycles access.\n                                   domain:bus:dev.fn=0000:04:00.0 addr.reg=88 data.reg=92\n                                   Chip revision is: 00\n\n\n\nThe /dev/mst/ path is the path to the device:\n\n\n/dev/mst/mt4119_pciconf0\n\n\n\n\n\n\n\nQuery the status of the device to determine whether SR-IOV is enabled, and how many virtual functions are configured.\n\n\nRun:\n\n\nmlxconfig -d <pci_device_path> q\n\n\n\nHere, <pci_device_path> is the path to the PCI device determined in the previous step. Ex:\n\n\n/dev/mst/mt4119_pciconf0\n\n\n\nYou should expect to see output similar to the following:\n\n\nDevice #1:\n----------\n\nDevice type:    ConnectX5       \nName:           MCX556A-ECA_Ax  \nDescription:    ConnectX-5 VPI adapter card; EDR IB (100Gb/s) and 100GbE; dual-port QSFP28; PCIe3.0 x16; tall bracket; ROHS R6\nDevice:         /dev/mst/mt4119_pciconf0\n\nConfigurations:                              Next Boot\n         MEMIC_BAR_SIZE                      0               \n         MEMIC_SIZE_LIMIT                    _256KB(1)       \n         HOST_CHAINING_MODE                  DISABLED(0)\n         ...\n         NUM_OF_VFS                          120\n         ...\n         SRIOV_EN                            True(1)\n         ...\n\n\n\nThe lines of interest to us are:\n\n\nSRIOV_EN                            True(1)\n\n\n\nWhich indicates whether or not SR-IOV has been enabled on the RDMA card.\n\n\nAnd:\n\n\nNUM_OF_VFS                          120\n\n\n\nWhich indicates how many SR-IOV virtual functions have been configured on the card.\n\n\nWe want to ensure that SR-IOV is enabled, and the number of virtual functions is configured to the largest amount the card will support.\n\n\n\n\n\n\nEnable SR-IOV and confuigure number of VFs.\n\n\nRun:\n\n\nmlxconfig -d <pci_device_path> set SRIOV_EN=1 NUM_OF_VFS=<max_num_vfs>\n\n\n\nHere, <pci_device_path> is the path determined in step 3, and <max_num_vfs> is the highest number of virtual functions that the RDMA hardware card supports. This can be found in the documentation for that card (this can typically be found in the firmware manual).\n\n\nFor example:\n\n\nmlxconfig -d /dev/mst/mt4119_pciconf0 set SRIOV_EN=1 NUM_OF_VFS=120\n\n\n\nChoose 'yes' when asked whether to apply the configuration.\n\n\n\n\n\n\nReboot the machine.\n\n\n\n\n\n\nVerify that the modification worked correctly.\n\n\nRun:\n\n\nsudo mst start\nsudo mst status\nmlxconfig -d <pci_device_path> q | egrep 'NUM_OF_VFS|SRIOV_EN'\n\n\n\nEnsure that the output of the last command matches the changes you have made prior to rebooting.\n\n\n\n\n\n\nRDMA Hardware Daemon Set\n\u00b6\n\n\nThis section covers the installation of the RDMA Hardware Daemon Set onto all of the worker nodes in the Kubernetes cluster.\n\n\n\n\n\n\nUse Kubernetes to deploy the Daemon Set to all the nodes in the cluster.\n\n\nOn the master node of the Kubernetes cluster, run:\n\n\nkubectl apply -f <rdma_daemonset_yaml>\n\n\n\nWhere <rdma_daemonset_yaml> is a YAML file that specifies the details of the Daemon Set. This file can be found at:\n\n\nhttps://github.com/rit-k8s-rdma/rit-k8s-rdma-ds/blob/master/rdma-ds.yaml\n\n\n\nApplying this Daemon Set configuration on the Kubernetes cluster will cause each worker node to pull container images built as part of this project from our \nDocker Hub repository\n.\n\n\nIf you would like to build this Docker image yourself, the instructions are available within the \nRDMA Hardware Daemon Set repository\n.\n\n\n\n\n\n\nVerify that the RDMA Hardware Daemon Set is running on each worker node in the cluster.\n\n\nOn the master node of the Kubernetes cluster, run:\n\n\nkubectl get pods -o wide --namespace kube-system\n\n\n\nWithin the output for this command, you should see several lines with the name: \nrdma-ds-*\n (one for each worker node in the cluster). The \nstatus\n column of each of these pods will show \nInit:0/1\n while the pod is performing hardware enumeration and initialization of the SR-IOV enabled RDMA hardware. One this has completed (it may take several minutes if there are a large number of virtual functions configured on a host), the status of the pods should switch to \nRunning\n.\n\n\n\n\n\n\nVerify that the RDMA Hardware Daemon Set's REST endpoint is running.\n\n\nOnce the status of each hardware daemon set pod, as inspected in the previous step, is \nRunning\n, the REST endpoint should be available to query. During the normal operation of the system, this would be done by the scheduler extension during the scheduling of a new pod. However, we can perform this process manually using 'curl' or a web browser.\n\n\nFirst, determine the IP address or hostname of the worker node whose RDMA Hardware Daemon Set you would like to test. Then, in a web browser, navigate to:\n\n\nhttp://<IP_OR_HOSTNAME>:54005/getpfs\n\n\n\nWhere <IP_OR_HOSTNAME> is the IP or hostname of the worker node whose daemon set you would like to test, and 54005 is the port that daemon set is listening on (54005 is the default value at the time of writing).\n\n\nYou should see output that resembles the following:\n\n\n[\n    {\n        \"name\":\"enp4s0f0\",\n        \"used_tx_rate\":0,\n        \"capacity_tx_rate\":100000,\n        \"used_vfs\":0,\n        \"capacity_vfs\":120,\n        \"vfs\":[\n            {\"vf\":0, \"mac\":\"7a:90:db:7b:30:ac\", \"vlan\":0, \"qos\":0, \"vlan_proto\": \"N/A\", \"spoof_check\":\"OFF\", \"trust\":\"ON\", \"link_state\":\"Follow\", \"min_tx_rate\":0, \"max_tx_rate\":0, \"vgt_plus\":\"OFF\", \"rate_group\":0, \"allocated\":false},\n            {\"vf\":1, \"mac\":\"c6:26:fe:e2:4e:95\", \"vlan\":0, \"qos\":0, \"vlan_proto\":\"N/A\", \"spoof_check\":\"OFF\", \"trust\":\"ON\", \"link_state\":\"Follow\", \"min_tx_rate\":0, \"max_tx_rate\":0, \"vgt_plus\":\"OFF\", \"rate_group\":0, \"allocated\":false},\n            ...\n        ]\n    },\n    {\n        \"name\":\"enp4s0f1\",\n        \"used_tx_rate\":0,\n        \"capacity_tx_rate\":100000,\n        \"used_vfs\":0,\n        \"capacity_vfs\":120,\n        \"vfs\":[\n            {\"vf\":0, \"mac\":\"02:b9:9a:99:9e:ac\", \"vlan\":0, \"qos\":0, \"vlan_proto\":\"N/A\", \"spoof_check\":\"OFF\", \"trust\":\"ON\", \"link_state\":\"Follow\", \"min_tx_rate\":0, \"max_tx_rate\":0, \"vgt_plus\":\"OFF\", \"rate_group\":0, \"allocated\":false},\n            {\"vf\":1, \"mac\":\"ea:99:de:00:e8:8b\", \"vlan\":0,\"qos\":0, \"vlan_proto\":\"N/A\", \"spoof_check\":\"OFF\", \"trust\":\"ON\", \"link_state\":\"Follow\", \"min_tx_rate\":0, \"max_tx_rate\":0, \"vgt_plus\":\"OFF\", \"rate_group\":0, \"allocated\":false},\n            ...\n        ]\n    },\n    ...\n]\n\n\n\nIf the RDMA Hardware Daemon Set endpoint responds, and the hardware information presented in the list it returns accurately reflects the state and details of the RDMA hardware on its node, then it is working correctly.\n\n\nRespeat this process for every node in the cluster to verify that each instance of the daemon set is working correctly.\n\n\n\n\n\n\nScheduler Extension\n\u00b6\n\n\nThis section covers the installation of the Scheduler Extension component.\n\n\n\n\n\n\nInstall and run the scheduler extension Docker container on the master node of the Kubernetes cluster.\n\n\nRun\n\n\ndocker run -d --rm --name ritk8srdma-scheduler-entension --network host ritk8srdma/rit-k8s-rdma-scheduler-extender\n\n\n\nThis will pull the Docker image for the scheduler extension from our Docker Hub repository and run it.\n\n\nIf you would like to build the scheduler extension docker image yourself, the instructions are available within the \nscheduler extension repository\n.\n\n\n\n\n\n\nModify the configuration of the core Kubernetes scheduler to register the scheduler extension.\n\n\nOn the master node of the Kubernetes cluster, edit or add the file /etc/kubernetes/scheduler-policy-config.json to register the scheduler extension. The following entry should be added to the 'extenders' list within that file:\n\n\n{\n    \"urlPrefix\": \"http://127.0.0.1:8888/scheduler\",\n    \"filterVerb\": \"rdma_scheduling\",\n    \"bindVerb\": \"\",\n    \"enableHttps\": false,\n    \"nodeCacheCapable\": false,\n    \"ignorable\": false\n}\n\n\n\nHere, the IP address/port combination of 127.0.0.1 and 8888 is used because the scheduler extension is running on the same node as the core Kubernetes scheduler (the master node of the Kubernetes cluster), and listening on port 8888. If the extension is run elsewhere or listening on a different port, the 'urlPrefix' parameter should be editted accordingly.\n\n\nAn example version of this file is available in the \nscheduler extension repository\n.\n\n\n\n\n\n\nEnsure the core Kubernetes scheduler is using the configuration file where the scheduler extension is registered.\n\n\nEdit the file /etc/kubernetes/manifests/kube-scheduler.yaml on the master node of the Kubernetes cluster. Add the following volume to the pod definition if it does not exist (place the definition within the existing 'volumes' section if one exists):\n\n\nvolumes:\n- hostPath:\n    path: /etc/kubernetes/scheduler-policy-config.json\n    type: FileOrCreate\n  name: scheduler-policy-config\n\n\n\nAdd the following directive to the command to be run in the kube-scheduler container:\n\n\n--policy-config-file=/etc/kubernetes/scheduler-policy-config.json\n\n\n\nOverall, the whole file should look like:\n\n\napiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    scheduler.alpha.kubernetes.io/critical-pod: \"\"\n  creationTimestamp: null\n  labels:\n    component: kube-scheduler\n    tier: control-plane\n  name: kube-scheduler\n  namespace: kube-system\nspec:\n  containers:\n  - command:\n    - kube-scheduler\n    - --address=127.0.0.1\n    - --kubeconfig=/etc/kubernetes/scheduler.conf\n    - --policy-config-file=/etc/kubernetes/scheduler-policy-config.json\n    - --leader-elect=true\n    image: k8s.gcr.io/kube-scheduler:v1.13.5\n    imagePullPolicy: IfNotPresent\n    livenessProbe:\n      failureThreshold: 8\n      httpGet:\n        host: 127.0.0.1\n        path: /healthz\n        port: 10251\n        scheme: HTTP\n      initialDelaySeconds: 15\n      timeoutSeconds: 15\n    name: kube-scheduler\n    resources:\n      requests:\n        cpu: 100m\n    volumeMounts:\n    - mountPath: /etc/kubernetes/scheduler.conf\n      name: kubeconfig\n      readOnly: true\n    - mountPath: /etc/kubernetes/scheduler-policy-config.json\n      name: scheduler-policy-config\n      readOnly: true\n  hostNetwork: true\n  priorityClassName: system-cluster-critical\n  volumes:\n  - hostPath:\n      path: /etc/kubernetes/scheduler.conf\n      type: FileOrCreate\n    name: kubeconfig\n  - hostPath:\n      path: /etc/kubernetes/scheduler-policy-config.json\n      type: FileOrCreate\n    name: scheduler-policy-config\nstatus: {}\n\n\n\n\n\n\n\nEnsure that the scheduler extension has started up correctly.\n\n\nRun \ndocker logs ritk8srdma-scheduler-entension\n on the node where the scheduler extension is running. The output should include the following line at the top of its output:\n\n\nYYYY/MM/DD HH:MM:SS RDMA scheduler extender listening on port:  <port_number>\n\n\n\nThis command can be run whenever necessary to view the logging output from the scheduler extension.\n\n\n\n\n\n\nCNI Plugin\n\u00b6\n\n\nThis section covers the installation of the CNI plugin on each RDMA-enabled worker node in the Kubernetes cluster.\n\n\n\n\n\n\nInstall the CNI plugin executable.\n\n\nCopy the 'rit-k8s-rdma-cni-linux-amd64' executable from \nthe releases page of the repository\n. Place it in /opt/cni/bin/ on each RDMA-enabled worker node in the Kubernetes cluster.\n\n\n\n\n\n\nInstall the CNI plugin configuration file.\n\n\nCopy the file '10-ritk8srdma-cni.conf' from \nthe releases page of the repository\n. Place it in /etc/cni/net.d/ on each RDMA-enabled worker node in the Kubernetes cluster. This configuration file can be edited to fit the needs of your environment. Ensure that this file is the first one (lexicographically) within that directory. Kubernetes always uses the CNI configuration that comes first lexicographically within this directory.\n\n\nWithin this file, the 'type' parameter specifies the name of the CNI executable that will be run when a pod is deployed. This name should match the name of the executable installed during step 1.\n\n\n\n\n\n\nTo compile the CNI plugin binary yourself, checkout the \nCNI repository\n. Enter the 'sriov' directory within the checked-out copy of the repository, then run \ngo install\n. The binary should then be created in the appropriate Golang bin directory.\n\n\nDummy Device Plugin\n\u00b6\n\n\nThis section covers the installation of the Dummy Device Plugin onto all of the worker nodes in the Kubernetes cluster.\n\n\n\n\n\n\nUse Kubernetes to deploy the dummy device plugin to all the nodes in the cluster.\n\n\nOn the master node of the Kubernetes cluster, run:\n\n\nkubectl apply -f <dummy_plugin_yaml>\n\n\n\nWhere <dummy_plugin_yaml> is a YAML file that specifies the details of the Dummy Device Plugin. This file can be found at:\n\n\nhttps://github.com/rit-k8s-rdma/rit-k8s-rdma-dummy-device-plugin/blob/master/rdma-dummy-dp-ds.yaml\n\n\n\nApplying this configuration on the Kubernetes cluster will cause each worker node to pull container images built as part of this project from our \nDocker Hub repository\n. These images contain the files necessary to run the Dummy Device Plugin.\n\n\nIf you would like to build the Dummy Device Plugin Docker image yourself, the instructions are available within the \nDummy Device Plugin repository\n.\n\n\n\n\n\n\nVerify that the Dummy Device Plugin is running on each worker node in the cluster.\n\n\nOn the master node of the Kubernetes cluster, run:\n\n\nkubectl get pods -o wide --namespace kube-system\n\n\n\nWithin the output for this command, you should see several lines with the name: \nrdma-dummy-dp-ds-*\n (one for each worker node in the cluster). The \nstatus\n column of each of these pods should show \nRunning\n.\n\n\n\n\n\n\nPod YAML Changes\n\u00b6\n\n\nTo take advantage of RDMA within a Kubernetes pod, that pod's definition (YAML file) will need to be updated to specify the RDMA interfaces that it requires. This involves the following steps:\n\n\n\n\n\n\n\nAdd the \nrdma_interfaces_required\n directive to the pod's metadata annotations:\n\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod1\n  annotations:\n    rdma_interfaces_required: '[\n        {\"min_tx_rate\": 15000, \"max_tx_rate\": 20000},\n        {\"min_tx_rate\": 5000},\n        {}\n    ]'\nspec:\n...\n\n\n\nThe value of this annotation should be a JSON-formatted string that contains a list of RDMA interfaces needed by the pod, as well as the bandwidth limitations and reservations for each of those interfaces. In this case \nmin_tx_rate\n specifies an amount of bandwidth that should be reserved for the pod to use exclusively through a specific RDMA interface, while \nmax_tx_rate\n sets a cap on the amount of bandwidth that can used by a pod through an interface. Either or both of these properties can be omitted if you do not need a bandwidth cap/reservation. In the example above, three RDMA interfaces are requested by a pod: the first sets both properties, the second has only a bandwidth reservation, and the third has no limit nor any reserved bandwidth. The numbers used are in units of megabits of bandwidth per second (Mb/S).\n\n\n\n\n\n\nAdd a request for a \n/dev/infiniband/\n mount to each container that will need access to RDMA interfaces:\n\n\n...\n  containers:\n  - image: mellanoxubuntudocker:latest\n    name: mofed-test-ctr1\n    resources:\n      limits:\n        rdma-sriov/dev-infiniband-mount: 1\n...\n\n\n\nThe line \nrdma-sriov/dev-infiniband-mount: 1\n indicates that the container requires privleged access to the \n/dev/infiniband\n directory. The quantity specified should be 1 (technically, the dummy device plugin is advertising an infinite amount of this resource type, but only one is needed to provide the mount point to the container).\n\n\n\n\n\n\nMake sure the container images being deployed in the pod contain the necessary RDMA libraries. This can be done by utilizing Mellanox's 'mellanoxubuntudocker:latest' Docker container image (and/or using this as a base to build other containers).\n\n\n\n\n\n\nAn example of a complete pod configuration file is available from our \ncommon repository\n.\n\n\nTesting and Verification\n\u00b6\n\n\nIn order to verify that the system is working correctly, we can perform several tests. These tests will involve performing actions that exercise some components of the system, then performing checks to ensure those components functioned correctly.\n\n\nDeploy a Pod\n\u00b6\n\n\nIn this test, we deploy a single pod within the cluster to ensure that basic connectivity exists between the scheduler extension, dummy device plugin, and CNI plugin.\n\n\n\n\n\n\nCreate a YAML file describing a pod to be deployed. Outfit this YAML file with the annotations mentioned in the 'Pod YAML Changes' section.\n\n\n\n\n\n\nRequest that Kubernetes deploy the pod onto the cluster. Running this command will indicate to Kubernetes that you would like to run the pod on one of the worker nodes in the cluster.\n\n\nkubectl create -f <pod_yaml_filename>\n\n\n\n\n\n\n\nView the status of the pod. Running this command will inform you where in the deployment process the pod is currently.\n\n\nkubectl get pods -o wide\n\n\n\nFind the entry in the list that matches the name of the pod in your YAML file from step 1. You want the pod's status to be \nRunning\n, though it may take several seconds to reach this state even when everything is working correctly, depending on the size of your cluster and the pod's resource requirements.\n\n\nIf the pod does not reach the \nRunning\n state after waiting for a while, you will need to perform some troubleshooting. First, observe more detailed information about the status of the pod by running:\n\n\nkubectl describe pod <POD_NAME>\n\n\n\nWhere <POD_NAME> is the name of the pod in your YAML file from step 1. Included in the information retrieved by this command is a the annotations used to request RDMA interface(s) for the pod, as well as the state of each container within the pod. When containers have a status other than \nRunning\n, they will include a reason, such as \nErrImagePull\n, which means the Docker image used to create the container couldn't be found on the node the pod was deployed on, or on Docker Hub. Also included in the output from this command is a list of 'Events' that have occured for the pod. If this events log contains a message like:\n\n\nPost http://127.0.0.1:8888/scheduler/rdma_scheduling: dial tcp 127.0.0.1:8888: connect: connection refused\n\n\n\nThen the scheduler extension could not be contacted by the core Kubernetes scheduler. Make sure the scheduler is running, and is configured to listen on the port the scheduler extension is trying to reach.\n\n\nAlternatively, if a message like the following appears in the event log:\n\n\n<N> RDMA Scheduler Extension: Unable to collect information on available RDMA resources for node.\n\n\n\nThen the scheduler extension experienced a timeout or a failed connection when attempting to contact the RDMA Hardware Daemon Set instanced on <N> of the nodes in the cluster. If this occurs, repeat the verification steps from the RDMA Hardware Daemon Set installation section. If this works, then verify that the port and URL being requested from within the scheduler extension match those that the daemon set is listening on. It may be helpful to look at the scheduler extension logs in this case.\n\n\nAnother possible message is the following:\n\n\n<N> RDMA Scheduler Extension: Node did not have enough free RDMA resources.\n\n\n\nWhich means that the scheduler extension received information that indicated the available RDMA VFs/bandwidth on <N> of the nodes in the cluster was not enough to satisfy the pod's requirements. If this seems incorrect, repeat step 3 of the RDMA Hardware Daemon Set install section, and inspect the amount of PFs, Bandwidth, and VFs available are correct for the node (and can satisfy what the pod is requesting).\n\n\nYet another message that may show up in the events log for a pod is the following:\n\n\n<N> RDMA Scheduler Extension: 'rdma_interfaces_required' field in pod YAML file is malformatted.\n\n\n\nThis simply means that the value in the \nrdma_interfaces_required\n annotation for the pod is not a valid JSON string. Simply delete the pending pod, fix the error, and re-deploy. It may be helpful to look at the scheduler extension logs in this case as well.\n\n\nAn error like:\n\n\n<N> Insufficient rdma-sriov/dev-infiniband-mount.\n\n\n\nComes from not having the dummy device plugin installed and configured correctly. View the status of its pods with \nkubectl get pods -o wide --namespace kube-system\n, and view the log files of each instance by executing \ndocker logs\n on the right container on each worker node.\n\n\nFinally, a few other errors specific to our system may come from the CNI plugin. These will be marked as such, but are also unexpected if the CNI plugin is installed at all. In order to view the logs for a specific instance of our CNI plugin that appears to have an issue, search the relevant node's system log for messages that begin with \nRIT-CNI\n.\n\n\n\n\n\n\nTest connectivity between two pods\n\u00b6\n\n\nIn order to ensure that the system has correctly provisioned pods with RDMA interfaces and set the correct bandwidth limits on those interfaces, we can run two pods on separate nodes in the Kubernetes cluster, then perform a bandwidth test between them.\n\n\n\n\n\n\nCreate two pod YAML files with the necessary information to request at least one RDMA interface. Add node selectors for this test to ensure that the two pods are deployed to two different nodes within the cluster (naturally, the nodes chosen should have enough RDMA resources available to satisfy the rquirements of the pods, the scheduler extension will prevent the pods from being deployed otherwise). Also, ensure the container images used for these pods contain the 'ib_send_bw' RDMA testing utility, as well as the necessary RDMA libraries.\n\n\n\n\n\n\nDeploy the two pods onto their respective nodes in the cluster. Follow the instructions from the 'Deploy a Pod' section for troubleshooting.\n\n\n\n\n\n\nOn the node to which the first pod has been deployed, find the Docker container running within that pod using \ndocker ps\n. Execute a shell within that container by running:\n\n\ndocker exec -ti <CONTAINER_ID> /bin/bash\n\n\n\nWhere <CONTAINER_ID> is the ID of the container that you found by running \ndocker ps\n.\n\n\n\n\n\n\nWhile in a shell within that container, perform the following actions:\n\n\nGet the pods IP address (use the IP for the eth0 interface):\n\n\nifconfig\n\n\n\nGet the name of the RDMA adapter for the VF that was allocated to the pod's eth0 interface:\n\n\nibdev2netdev\n\n\n\nRun the receiving (server) side of the bandwidth testing application:\n\n\nib_send_bw -d <RDMA_ADAPTER_NAME> -i 1 -F --report_gbits --run_infinitely\n\n\n\nWhere <RDMA_ADAPTER_NAME> is the name of the form 'mlx5_N' which was connected to the interface whose IP address you found when running \nifconfig\n.\n\n\nThis application is now waiting for an incoming connection. We will run the sending/client end of the test from a container within the other pod that we deployed onto another node in the cluster.\n\n\n\n\n\n\nOn the node to which the second pod has been deployed, find a Docker container running within that pod and execute a shell within it (follow the same process as you did for the first pod).\n\n\n\n\n\n\nWithin the shell running in the container from the second pod, perform the following actions:\n\n\nGet the name of the RDMA adapter for the VF that was allocated to the pod's eth0 interface:\n\n\nibdev2netdev\n\n\n\nRun the sending (client) side of the bandwidth testing application:\n\n\nib_send_bw -d <RDMA_ADAPTER_NAME> -i 1 -F --report_gbits <SERVER_IP> --run_infinitely\n\n\n\nWhere <RDMA_ADAPTER_NAME> is the name of the form 'mlx5_N' which was connected to the eth0 interface, and <SERVER_IP> is the IP address you found for the interface within the other (first) pod in step 4.\n\n\nThis command should display a table of bandwidth measurements that is updated periodically as it continues to run. The measurements displayed in the \nBW average[Gb/sec]\n column should all be at or below the bandwidth limit you set on the sending pod within its YAML file. If this is the case, then bandwidth limitation is working. A similar test can be run for bandwidth reservation, using additional pairs of pods that compete for bandwidth with the one that has the reservation.",
            "title": "Install"
        },
        {
            "location": "/install/#install",
            "text": "",
            "title": "Install"
        },
        {
            "location": "/install/#prerequisites",
            "text": "Before installing this system, you should have a working Kubernetes cluster set up. The following prerequisites must be met:\n - Kubernetes version 1.13\n - Golang version 1.12\n   - Version 1.12 or greater is necessary to compile the software components of the system.\n - Mellanox OFED version 4.6-1.0.1.1 or greater\n   - The firmware on Mellanox RDMA cards should be updated to the latest available version.",
            "title": "Prerequisites"
        },
        {
            "location": "/install/#hardware_setup",
            "text": "This section covers the configuration of Mellanox RDMA hardware in preparation for using SR-IOV.   Enable SR-IOV in the BIOS of an machine with RDMA hardware installed.   Load the Mellanox driver modules for making configuration changes to the RDMA hardware.  Run:  sudo mst start  You should expect to see output similar to the following:  Starting MST (Mellanox Software Tools) driver set\nLoading MST PCI module - Success\nLoading MST PCI configuration module - Success\nCreate devices\nUnloading MST PCI module (unused) - Success    Determine the path to the PCI device for the RDMA hardware card.  Run:  sudo mst status  You should expect to see output similar to the following:  MST modules:\n------------\n    MST PCI module is not loaded\n    MST PCI configuration module loaded\n\nMST devices:\n------------\n/dev/mst/mt4119_pciconf0         - PCI configuration cycles access.\n                                   domain:bus:dev.fn=0000:04:00.0 addr.reg=88 data.reg=92\n                                   Chip revision is: 00  The /dev/mst/ path is the path to the device:  /dev/mst/mt4119_pciconf0    Query the status of the device to determine whether SR-IOV is enabled, and how many virtual functions are configured.  Run:  mlxconfig -d <pci_device_path> q  Here, <pci_device_path> is the path to the PCI device determined in the previous step. Ex:  /dev/mst/mt4119_pciconf0  You should expect to see output similar to the following:  Device #1:\n----------\n\nDevice type:    ConnectX5       \nName:           MCX556A-ECA_Ax  \nDescription:    ConnectX-5 VPI adapter card; EDR IB (100Gb/s) and 100GbE; dual-port QSFP28; PCIe3.0 x16; tall bracket; ROHS R6\nDevice:         /dev/mst/mt4119_pciconf0\n\nConfigurations:                              Next Boot\n         MEMIC_BAR_SIZE                      0               \n         MEMIC_SIZE_LIMIT                    _256KB(1)       \n         HOST_CHAINING_MODE                  DISABLED(0)\n         ...\n         NUM_OF_VFS                          120\n         ...\n         SRIOV_EN                            True(1)\n         ...  The lines of interest to us are:  SRIOV_EN                            True(1)  Which indicates whether or not SR-IOV has been enabled on the RDMA card.  And:  NUM_OF_VFS                          120  Which indicates how many SR-IOV virtual functions have been configured on the card.  We want to ensure that SR-IOV is enabled, and the number of virtual functions is configured to the largest amount the card will support.    Enable SR-IOV and confuigure number of VFs.  Run:  mlxconfig -d <pci_device_path> set SRIOV_EN=1 NUM_OF_VFS=<max_num_vfs>  Here, <pci_device_path> is the path determined in step 3, and <max_num_vfs> is the highest number of virtual functions that the RDMA hardware card supports. This can be found in the documentation for that card (this can typically be found in the firmware manual).  For example:  mlxconfig -d /dev/mst/mt4119_pciconf0 set SRIOV_EN=1 NUM_OF_VFS=120  Choose 'yes' when asked whether to apply the configuration.    Reboot the machine.    Verify that the modification worked correctly.  Run:  sudo mst start\nsudo mst status\nmlxconfig -d <pci_device_path> q | egrep 'NUM_OF_VFS|SRIOV_EN'  Ensure that the output of the last command matches the changes you have made prior to rebooting.",
            "title": "Hardware Setup"
        },
        {
            "location": "/install/#rdma_hardware_daemon_set",
            "text": "This section covers the installation of the RDMA Hardware Daemon Set onto all of the worker nodes in the Kubernetes cluster.    Use Kubernetes to deploy the Daemon Set to all the nodes in the cluster.  On the master node of the Kubernetes cluster, run:  kubectl apply -f <rdma_daemonset_yaml>  Where <rdma_daemonset_yaml> is a YAML file that specifies the details of the Daemon Set. This file can be found at:  https://github.com/rit-k8s-rdma/rit-k8s-rdma-ds/blob/master/rdma-ds.yaml  Applying this Daemon Set configuration on the Kubernetes cluster will cause each worker node to pull container images built as part of this project from our  Docker Hub repository .  If you would like to build this Docker image yourself, the instructions are available within the  RDMA Hardware Daemon Set repository .    Verify that the RDMA Hardware Daemon Set is running on each worker node in the cluster.  On the master node of the Kubernetes cluster, run:  kubectl get pods -o wide --namespace kube-system  Within the output for this command, you should see several lines with the name:  rdma-ds-*  (one for each worker node in the cluster). The  status  column of each of these pods will show  Init:0/1  while the pod is performing hardware enumeration and initialization of the SR-IOV enabled RDMA hardware. One this has completed (it may take several minutes if there are a large number of virtual functions configured on a host), the status of the pods should switch to  Running .    Verify that the RDMA Hardware Daemon Set's REST endpoint is running.  Once the status of each hardware daemon set pod, as inspected in the previous step, is  Running , the REST endpoint should be available to query. During the normal operation of the system, this would be done by the scheduler extension during the scheduling of a new pod. However, we can perform this process manually using 'curl' or a web browser.  First, determine the IP address or hostname of the worker node whose RDMA Hardware Daemon Set you would like to test. Then, in a web browser, navigate to:  http://<IP_OR_HOSTNAME>:54005/getpfs  Where <IP_OR_HOSTNAME> is the IP or hostname of the worker node whose daemon set you would like to test, and 54005 is the port that daemon set is listening on (54005 is the default value at the time of writing).  You should see output that resembles the following:  [\n    {\n        \"name\":\"enp4s0f0\",\n        \"used_tx_rate\":0,\n        \"capacity_tx_rate\":100000,\n        \"used_vfs\":0,\n        \"capacity_vfs\":120,\n        \"vfs\":[\n            {\"vf\":0, \"mac\":\"7a:90:db:7b:30:ac\", \"vlan\":0, \"qos\":0, \"vlan_proto\": \"N/A\", \"spoof_check\":\"OFF\", \"trust\":\"ON\", \"link_state\":\"Follow\", \"min_tx_rate\":0, \"max_tx_rate\":0, \"vgt_plus\":\"OFF\", \"rate_group\":0, \"allocated\":false},\n            {\"vf\":1, \"mac\":\"c6:26:fe:e2:4e:95\", \"vlan\":0, \"qos\":0, \"vlan_proto\":\"N/A\", \"spoof_check\":\"OFF\", \"trust\":\"ON\", \"link_state\":\"Follow\", \"min_tx_rate\":0, \"max_tx_rate\":0, \"vgt_plus\":\"OFF\", \"rate_group\":0, \"allocated\":false},\n            ...\n        ]\n    },\n    {\n        \"name\":\"enp4s0f1\",\n        \"used_tx_rate\":0,\n        \"capacity_tx_rate\":100000,\n        \"used_vfs\":0,\n        \"capacity_vfs\":120,\n        \"vfs\":[\n            {\"vf\":0, \"mac\":\"02:b9:9a:99:9e:ac\", \"vlan\":0, \"qos\":0, \"vlan_proto\":\"N/A\", \"spoof_check\":\"OFF\", \"trust\":\"ON\", \"link_state\":\"Follow\", \"min_tx_rate\":0, \"max_tx_rate\":0, \"vgt_plus\":\"OFF\", \"rate_group\":0, \"allocated\":false},\n            {\"vf\":1, \"mac\":\"ea:99:de:00:e8:8b\", \"vlan\":0,\"qos\":0, \"vlan_proto\":\"N/A\", \"spoof_check\":\"OFF\", \"trust\":\"ON\", \"link_state\":\"Follow\", \"min_tx_rate\":0, \"max_tx_rate\":0, \"vgt_plus\":\"OFF\", \"rate_group\":0, \"allocated\":false},\n            ...\n        ]\n    },\n    ...\n]  If the RDMA Hardware Daemon Set endpoint responds, and the hardware information presented in the list it returns accurately reflects the state and details of the RDMA hardware on its node, then it is working correctly.  Respeat this process for every node in the cluster to verify that each instance of the daemon set is working correctly.",
            "title": "RDMA Hardware Daemon Set"
        },
        {
            "location": "/install/#scheduler_extension",
            "text": "This section covers the installation of the Scheduler Extension component.    Install and run the scheduler extension Docker container on the master node of the Kubernetes cluster.  Run  docker run -d --rm --name ritk8srdma-scheduler-entension --network host ritk8srdma/rit-k8s-rdma-scheduler-extender  This will pull the Docker image for the scheduler extension from our Docker Hub repository and run it.  If you would like to build the scheduler extension docker image yourself, the instructions are available within the  scheduler extension repository .    Modify the configuration of the core Kubernetes scheduler to register the scheduler extension.  On the master node of the Kubernetes cluster, edit or add the file /etc/kubernetes/scheduler-policy-config.json to register the scheduler extension. The following entry should be added to the 'extenders' list within that file:  {\n    \"urlPrefix\": \"http://127.0.0.1:8888/scheduler\",\n    \"filterVerb\": \"rdma_scheduling\",\n    \"bindVerb\": \"\",\n    \"enableHttps\": false,\n    \"nodeCacheCapable\": false,\n    \"ignorable\": false\n}  Here, the IP address/port combination of 127.0.0.1 and 8888 is used because the scheduler extension is running on the same node as the core Kubernetes scheduler (the master node of the Kubernetes cluster), and listening on port 8888. If the extension is run elsewhere or listening on a different port, the 'urlPrefix' parameter should be editted accordingly.  An example version of this file is available in the  scheduler extension repository .    Ensure the core Kubernetes scheduler is using the configuration file where the scheduler extension is registered.  Edit the file /etc/kubernetes/manifests/kube-scheduler.yaml on the master node of the Kubernetes cluster. Add the following volume to the pod definition if it does not exist (place the definition within the existing 'volumes' section if one exists):  volumes:\n- hostPath:\n    path: /etc/kubernetes/scheduler-policy-config.json\n    type: FileOrCreate\n  name: scheduler-policy-config  Add the following directive to the command to be run in the kube-scheduler container:  --policy-config-file=/etc/kubernetes/scheduler-policy-config.json  Overall, the whole file should look like:  apiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    scheduler.alpha.kubernetes.io/critical-pod: \"\"\n  creationTimestamp: null\n  labels:\n    component: kube-scheduler\n    tier: control-plane\n  name: kube-scheduler\n  namespace: kube-system\nspec:\n  containers:\n  - command:\n    - kube-scheduler\n    - --address=127.0.0.1\n    - --kubeconfig=/etc/kubernetes/scheduler.conf\n    - --policy-config-file=/etc/kubernetes/scheduler-policy-config.json\n    - --leader-elect=true\n    image: k8s.gcr.io/kube-scheduler:v1.13.5\n    imagePullPolicy: IfNotPresent\n    livenessProbe:\n      failureThreshold: 8\n      httpGet:\n        host: 127.0.0.1\n        path: /healthz\n        port: 10251\n        scheme: HTTP\n      initialDelaySeconds: 15\n      timeoutSeconds: 15\n    name: kube-scheduler\n    resources:\n      requests:\n        cpu: 100m\n    volumeMounts:\n    - mountPath: /etc/kubernetes/scheduler.conf\n      name: kubeconfig\n      readOnly: true\n    - mountPath: /etc/kubernetes/scheduler-policy-config.json\n      name: scheduler-policy-config\n      readOnly: true\n  hostNetwork: true\n  priorityClassName: system-cluster-critical\n  volumes:\n  - hostPath:\n      path: /etc/kubernetes/scheduler.conf\n      type: FileOrCreate\n    name: kubeconfig\n  - hostPath:\n      path: /etc/kubernetes/scheduler-policy-config.json\n      type: FileOrCreate\n    name: scheduler-policy-config\nstatus: {}    Ensure that the scheduler extension has started up correctly.  Run  docker logs ritk8srdma-scheduler-entension  on the node where the scheduler extension is running. The output should include the following line at the top of its output:  YYYY/MM/DD HH:MM:SS RDMA scheduler extender listening on port:  <port_number>  This command can be run whenever necessary to view the logging output from the scheduler extension.",
            "title": "Scheduler Extension"
        },
        {
            "location": "/install/#cni_plugin",
            "text": "This section covers the installation of the CNI plugin on each RDMA-enabled worker node in the Kubernetes cluster.    Install the CNI plugin executable.  Copy the 'rit-k8s-rdma-cni-linux-amd64' executable from  the releases page of the repository . Place it in /opt/cni/bin/ on each RDMA-enabled worker node in the Kubernetes cluster.    Install the CNI plugin configuration file.  Copy the file '10-ritk8srdma-cni.conf' from  the releases page of the repository . Place it in /etc/cni/net.d/ on each RDMA-enabled worker node in the Kubernetes cluster. This configuration file can be edited to fit the needs of your environment. Ensure that this file is the first one (lexicographically) within that directory. Kubernetes always uses the CNI configuration that comes first lexicographically within this directory.  Within this file, the 'type' parameter specifies the name of the CNI executable that will be run when a pod is deployed. This name should match the name of the executable installed during step 1.    To compile the CNI plugin binary yourself, checkout the  CNI repository . Enter the 'sriov' directory within the checked-out copy of the repository, then run  go install . The binary should then be created in the appropriate Golang bin directory.",
            "title": "CNI Plugin"
        },
        {
            "location": "/install/#dummy_device_plugin",
            "text": "This section covers the installation of the Dummy Device Plugin onto all of the worker nodes in the Kubernetes cluster.    Use Kubernetes to deploy the dummy device plugin to all the nodes in the cluster.  On the master node of the Kubernetes cluster, run:  kubectl apply -f <dummy_plugin_yaml>  Where <dummy_plugin_yaml> is a YAML file that specifies the details of the Dummy Device Plugin. This file can be found at:  https://github.com/rit-k8s-rdma/rit-k8s-rdma-dummy-device-plugin/blob/master/rdma-dummy-dp-ds.yaml  Applying this configuration on the Kubernetes cluster will cause each worker node to pull container images built as part of this project from our  Docker Hub repository . These images contain the files necessary to run the Dummy Device Plugin.  If you would like to build the Dummy Device Plugin Docker image yourself, the instructions are available within the  Dummy Device Plugin repository .    Verify that the Dummy Device Plugin is running on each worker node in the cluster.  On the master node of the Kubernetes cluster, run:  kubectl get pods -o wide --namespace kube-system  Within the output for this command, you should see several lines with the name:  rdma-dummy-dp-ds-*  (one for each worker node in the cluster). The  status  column of each of these pods should show  Running .",
            "title": "Dummy Device Plugin"
        },
        {
            "location": "/install/#pod_yaml_changes",
            "text": "To take advantage of RDMA within a Kubernetes pod, that pod's definition (YAML file) will need to be updated to specify the RDMA interfaces that it requires. This involves the following steps:    Add the  rdma_interfaces_required  directive to the pod's metadata annotations:  apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod1\n  annotations:\n    rdma_interfaces_required: '[\n        {\"min_tx_rate\": 15000, \"max_tx_rate\": 20000},\n        {\"min_tx_rate\": 5000},\n        {}\n    ]'\nspec:\n...  The value of this annotation should be a JSON-formatted string that contains a list of RDMA interfaces needed by the pod, as well as the bandwidth limitations and reservations for each of those interfaces. In this case  min_tx_rate  specifies an amount of bandwidth that should be reserved for the pod to use exclusively through a specific RDMA interface, while  max_tx_rate  sets a cap on the amount of bandwidth that can used by a pod through an interface. Either or both of these properties can be omitted if you do not need a bandwidth cap/reservation. In the example above, three RDMA interfaces are requested by a pod: the first sets both properties, the second has only a bandwidth reservation, and the third has no limit nor any reserved bandwidth. The numbers used are in units of megabits of bandwidth per second (Mb/S).    Add a request for a  /dev/infiniband/  mount to each container that will need access to RDMA interfaces:  ...\n  containers:\n  - image: mellanoxubuntudocker:latest\n    name: mofed-test-ctr1\n    resources:\n      limits:\n        rdma-sriov/dev-infiniband-mount: 1\n...  The line  rdma-sriov/dev-infiniband-mount: 1  indicates that the container requires privleged access to the  /dev/infiniband  directory. The quantity specified should be 1 (technically, the dummy device plugin is advertising an infinite amount of this resource type, but only one is needed to provide the mount point to the container).    Make sure the container images being deployed in the pod contain the necessary RDMA libraries. This can be done by utilizing Mellanox's 'mellanoxubuntudocker:latest' Docker container image (and/or using this as a base to build other containers).    An example of a complete pod configuration file is available from our  common repository .",
            "title": "Pod YAML Changes"
        },
        {
            "location": "/install/#testing_and_verification",
            "text": "In order to verify that the system is working correctly, we can perform several tests. These tests will involve performing actions that exercise some components of the system, then performing checks to ensure those components functioned correctly.",
            "title": "Testing and Verification"
        },
        {
            "location": "/install/#deploy_a_pod",
            "text": "In this test, we deploy a single pod within the cluster to ensure that basic connectivity exists between the scheduler extension, dummy device plugin, and CNI plugin.    Create a YAML file describing a pod to be deployed. Outfit this YAML file with the annotations mentioned in the 'Pod YAML Changes' section.    Request that Kubernetes deploy the pod onto the cluster. Running this command will indicate to Kubernetes that you would like to run the pod on one of the worker nodes in the cluster.  kubectl create -f <pod_yaml_filename>    View the status of the pod. Running this command will inform you where in the deployment process the pod is currently.  kubectl get pods -o wide  Find the entry in the list that matches the name of the pod in your YAML file from step 1. You want the pod's status to be  Running , though it may take several seconds to reach this state even when everything is working correctly, depending on the size of your cluster and the pod's resource requirements.  If the pod does not reach the  Running  state after waiting for a while, you will need to perform some troubleshooting. First, observe more detailed information about the status of the pod by running:  kubectl describe pod <POD_NAME>  Where <POD_NAME> is the name of the pod in your YAML file from step 1. Included in the information retrieved by this command is a the annotations used to request RDMA interface(s) for the pod, as well as the state of each container within the pod. When containers have a status other than  Running , they will include a reason, such as  ErrImagePull , which means the Docker image used to create the container couldn't be found on the node the pod was deployed on, or on Docker Hub. Also included in the output from this command is a list of 'Events' that have occured for the pod. If this events log contains a message like:  Post http://127.0.0.1:8888/scheduler/rdma_scheduling: dial tcp 127.0.0.1:8888: connect: connection refused  Then the scheduler extension could not be contacted by the core Kubernetes scheduler. Make sure the scheduler is running, and is configured to listen on the port the scheduler extension is trying to reach.  Alternatively, if a message like the following appears in the event log:  <N> RDMA Scheduler Extension: Unable to collect information on available RDMA resources for node.  Then the scheduler extension experienced a timeout or a failed connection when attempting to contact the RDMA Hardware Daemon Set instanced on <N> of the nodes in the cluster. If this occurs, repeat the verification steps from the RDMA Hardware Daemon Set installation section. If this works, then verify that the port and URL being requested from within the scheduler extension match those that the daemon set is listening on. It may be helpful to look at the scheduler extension logs in this case.  Another possible message is the following:  <N> RDMA Scheduler Extension: Node did not have enough free RDMA resources.  Which means that the scheduler extension received information that indicated the available RDMA VFs/bandwidth on <N> of the nodes in the cluster was not enough to satisfy the pod's requirements. If this seems incorrect, repeat step 3 of the RDMA Hardware Daemon Set install section, and inspect the amount of PFs, Bandwidth, and VFs available are correct for the node (and can satisfy what the pod is requesting).  Yet another message that may show up in the events log for a pod is the following:  <N> RDMA Scheduler Extension: 'rdma_interfaces_required' field in pod YAML file is malformatted.  This simply means that the value in the  rdma_interfaces_required  annotation for the pod is not a valid JSON string. Simply delete the pending pod, fix the error, and re-deploy. It may be helpful to look at the scheduler extension logs in this case as well.  An error like:  <N> Insufficient rdma-sriov/dev-infiniband-mount.  Comes from not having the dummy device plugin installed and configured correctly. View the status of its pods with  kubectl get pods -o wide --namespace kube-system , and view the log files of each instance by executing  docker logs  on the right container on each worker node.  Finally, a few other errors specific to our system may come from the CNI plugin. These will be marked as such, but are also unexpected if the CNI plugin is installed at all. In order to view the logs for a specific instance of our CNI plugin that appears to have an issue, search the relevant node's system log for messages that begin with  RIT-CNI .",
            "title": "Deploy a Pod"
        },
        {
            "location": "/install/#test_connectivity_between_two_pods",
            "text": "In order to ensure that the system has correctly provisioned pods with RDMA interfaces and set the correct bandwidth limits on those interfaces, we can run two pods on separate nodes in the Kubernetes cluster, then perform a bandwidth test between them.    Create two pod YAML files with the necessary information to request at least one RDMA interface. Add node selectors for this test to ensure that the two pods are deployed to two different nodes within the cluster (naturally, the nodes chosen should have enough RDMA resources available to satisfy the rquirements of the pods, the scheduler extension will prevent the pods from being deployed otherwise). Also, ensure the container images used for these pods contain the 'ib_send_bw' RDMA testing utility, as well as the necessary RDMA libraries.    Deploy the two pods onto their respective nodes in the cluster. Follow the instructions from the 'Deploy a Pod' section for troubleshooting.    On the node to which the first pod has been deployed, find the Docker container running within that pod using  docker ps . Execute a shell within that container by running:  docker exec -ti <CONTAINER_ID> /bin/bash  Where <CONTAINER_ID> is the ID of the container that you found by running  docker ps .    While in a shell within that container, perform the following actions:  Get the pods IP address (use the IP for the eth0 interface):  ifconfig  Get the name of the RDMA adapter for the VF that was allocated to the pod's eth0 interface:  ibdev2netdev  Run the receiving (server) side of the bandwidth testing application:  ib_send_bw -d <RDMA_ADAPTER_NAME> -i 1 -F --report_gbits --run_infinitely  Where <RDMA_ADAPTER_NAME> is the name of the form 'mlx5_N' which was connected to the interface whose IP address you found when running  ifconfig .  This application is now waiting for an incoming connection. We will run the sending/client end of the test from a container within the other pod that we deployed onto another node in the cluster.    On the node to which the second pod has been deployed, find a Docker container running within that pod and execute a shell within it (follow the same process as you did for the first pod).    Within the shell running in the container from the second pod, perform the following actions:  Get the name of the RDMA adapter for the VF that was allocated to the pod's eth0 interface:  ibdev2netdev  Run the sending (client) side of the bandwidth testing application:  ib_send_bw -d <RDMA_ADAPTER_NAME> -i 1 -F --report_gbits <SERVER_IP> --run_infinitely  Where <RDMA_ADAPTER_NAME> is the name of the form 'mlx5_N' which was connected to the eth0 interface, and <SERVER_IP> is the IP address you found for the interface within the other (first) pod in step 4.  This command should display a table of bandwidth measurements that is updated periodically as it continues to run. The measurements displayed in the  BW average[Gb/sec]  column should all be at or below the bandwidth limit you set on the sending pod within its YAML file. If this is the case, then bandwidth limitation is working. A similar test can be run for bandwidth reservation, using additional pairs of pods that compete for bandwidth with the one that has the reservation.",
            "title": "Test connectivity between two pods"
        },
        {
            "location": "/introduction/",
            "text": "Introduction\n\u00b6\n\n\nThe main focus of this project was to enable bandwidth limiting and allow for multiple interfaces to be specified for RDMA interfaces on a per pod basis. The current Mellanox Solution has a number of pitfalls such as an inaccurate state between the CNI and the Device Plugin. The Device Plugin is managed by Kubernetes, so Kubernets decides which interface to allocate to a given container. This is not reflected accurately in the CNI which may give it a different interface. In the current solution a pod may have 3 containers that each request a single RDMA interface; in the solution the Device Plugin removes 3 RDMA interfaces from the available pool of interfaces, but the CNI only allocates a single interface; the state of the CNI and the Device Plugin are incorrect. One of the largest problems is that Mellanox treats RDMA interfaces as a container specified resource, when in reality network namespaces are shared across pods, so any container in a pod has access to all of the same interfaces. This becomes a problem because when specifying Device Resources within a pod yaml, they are on a per container basis. For all of the reasons above the Device Plugin approach was abandoned because it could not accommodate our goals and instead we opted for a new architecture.\n\n\nArchitecture\n\u00b6\n\n\nThe main system architecture for our design can be seen below; the green color specifies our components for our design and the yellow color specify Kubernetes components:\n\n\n\n\nThe main workflow for how a pod would deployed in our system begins with a request for deploying a pod going to the master nodes Kubernetes Control Process. After that request is seen the Kubernetes Scheduler creates a list of possible nodes that the pod can be placed on based on requirements of the pods yaml. This list then gets sent to our \nScheduler Extension\n, which is in charge of deciding which nodes can support the RDMA requirements specified in the pods yaml.\n\n\nThe \nScheduler Extension\n contacts each nodes \nRDMA Hardware Daemon Set\n, which returns a JSON formatted list of information about a nodes RDMA VF's back to the \nScheduler Extension\n. The \nScheduler Extension\n than processes all of the information to find a valid node that can meet the minimum bandwidth requirements of each of the requested interfaces that is specified in the pods yaml; the list of nodes whether blank or empty is sent back to the Kubernetes Core Scheduler. If no node is valid after the scheduling calls an error is raised and the pod is not placed, this error can be seen with a Kubernetes describe command of why the pod was not placed.\n\n\nAssuming that the pod was able to placed on at least one valid node, the Kubelet process on the valid node gets called to setup the pod. During the pods setup process the (CNI)[components.md#cni] is called to setup the network of the pod. The (CNI)[components.md#cni] first contacts the \nRDMA Hardware Daemon Set\n on the node that is running to get an up to date list of the state of the node. It then runs the same algorithm that the \nScheduler Extension\n had run to find the correct placements of interfaces to meet the requirements of the bandwidth limitations. The (CNI)[components.md#cni] is atomic operation, so it either completes the setup of the pod or fails and rollbacks all changes made to any interfaces. Once the (CNI)[components.md#cni] finishes, the response is sent back to the Kubelet process of the node.\n\n\nLimitations\n\u00b6\n\n\nThere are a couple limitations when it comes to our solution:\n- Mellanox Vendor - the following has only been test on a Mellanox Card\n- Data Plane Development Kit (DPDK) - the Mellanox solution may work with DPDK, it has not been tested with our solution (changes to (CNI)[https://github.com/rit-k8s-rdma/rit-k8s-rdma-sriov-cni] required)\n- Shared RDMA Device - the Mellanox solution may work with a shared RDMA interface, it has not been tested with our solution (changes to (CNI)[https://github.com/rit-k8s-rdma/rit-k8s-rdma-sriov-cni] required)\n- Dummy Device Plugin - the current solution requires the use of Device Plugin to give access to \n/dev/infiniband\n for open issues in Kubernets that can be found \nhere\n and \nhere\n. The main problem is Kubernetes does not have the ideas of a \ndevice\n directory that \nDocker has \n--device\n.\n\n\nFuture Work\n\u00b6\n\n\n\n\nMore Vendors - making the solution more interface driven so it can be adapted to more vendors then just Mellanox.\n\n\nMigrating CNI - the CNI is currently at an older version and we had to bootstrap the newer one, it should be upgraded.\n\n\nScheduling - opening up the scheduler to be more adaptable to customizable scheduling algorithms.\n\n\n\n\nOur Approach in Short\n\u00b6\n\n\nProblems Addressed:\n - VF's are specified per container\n - Network plugin only ever gives one vf per pod, this means that the device plugin which tracks the amount of available VF's does not maintain an accurate count of VF's being used\n - The VF's selected by Kuberentes to be allocated, may not match those actually allocated by the network plugin\n\n\nSolution:\n - Device Plugin (RDMA)\n   - Changes to a DameonSet \n   - Stores the current state of the nodes VF resources\n   - Can be quierried through an API\n - Schedular extender\n   - Queries each DameonSet on each node and then filters the possible nodes that a pod can be deployed on based on resource requirements in the annotations for the pod\n - Network Plugin (SRIOV)\n   - Modify it to handle read pods meta-data\n     - Read the amount of VF's\n     - Read the bandwidth limitation on each VF\n   - Modify plugin to set the bandwidth limits from read metadata\n   - Add ability to set \nmin_tx_rate\n and \nmax_tx_rate",
            "title": "Introduction"
        },
        {
            "location": "/introduction/#introduction",
            "text": "The main focus of this project was to enable bandwidth limiting and allow for multiple interfaces to be specified for RDMA interfaces on a per pod basis. The current Mellanox Solution has a number of pitfalls such as an inaccurate state between the CNI and the Device Plugin. The Device Plugin is managed by Kubernetes, so Kubernets decides which interface to allocate to a given container. This is not reflected accurately in the CNI which may give it a different interface. In the current solution a pod may have 3 containers that each request a single RDMA interface; in the solution the Device Plugin removes 3 RDMA interfaces from the available pool of interfaces, but the CNI only allocates a single interface; the state of the CNI and the Device Plugin are incorrect. One of the largest problems is that Mellanox treats RDMA interfaces as a container specified resource, when in reality network namespaces are shared across pods, so any container in a pod has access to all of the same interfaces. This becomes a problem because when specifying Device Resources within a pod yaml, they are on a per container basis. For all of the reasons above the Device Plugin approach was abandoned because it could not accommodate our goals and instead we opted for a new architecture.",
            "title": "Introduction"
        },
        {
            "location": "/introduction/#architecture",
            "text": "The main system architecture for our design can be seen below; the green color specifies our components for our design and the yellow color specify Kubernetes components:   The main workflow for how a pod would deployed in our system begins with a request for deploying a pod going to the master nodes Kubernetes Control Process. After that request is seen the Kubernetes Scheduler creates a list of possible nodes that the pod can be placed on based on requirements of the pods yaml. This list then gets sent to our  Scheduler Extension , which is in charge of deciding which nodes can support the RDMA requirements specified in the pods yaml.  The  Scheduler Extension  contacts each nodes  RDMA Hardware Daemon Set , which returns a JSON formatted list of information about a nodes RDMA VF's back to the  Scheduler Extension . The  Scheduler Extension  than processes all of the information to find a valid node that can meet the minimum bandwidth requirements of each of the requested interfaces that is specified in the pods yaml; the list of nodes whether blank or empty is sent back to the Kubernetes Core Scheduler. If no node is valid after the scheduling calls an error is raised and the pod is not placed, this error can be seen with a Kubernetes describe command of why the pod was not placed.  Assuming that the pod was able to placed on at least one valid node, the Kubelet process on the valid node gets called to setup the pod. During the pods setup process the (CNI)[components.md#cni] is called to setup the network of the pod. The (CNI)[components.md#cni] first contacts the  RDMA Hardware Daemon Set  on the node that is running to get an up to date list of the state of the node. It then runs the same algorithm that the  Scheduler Extension  had run to find the correct placements of interfaces to meet the requirements of the bandwidth limitations. The (CNI)[components.md#cni] is atomic operation, so it either completes the setup of the pod or fails and rollbacks all changes made to any interfaces. Once the (CNI)[components.md#cni] finishes, the response is sent back to the Kubelet process of the node.",
            "title": "Architecture"
        },
        {
            "location": "/introduction/#limitations",
            "text": "There are a couple limitations when it comes to our solution:\n- Mellanox Vendor - the following has only been test on a Mellanox Card\n- Data Plane Development Kit (DPDK) - the Mellanox solution may work with DPDK, it has not been tested with our solution (changes to (CNI)[https://github.com/rit-k8s-rdma/rit-k8s-rdma-sriov-cni] required)\n- Shared RDMA Device - the Mellanox solution may work with a shared RDMA interface, it has not been tested with our solution (changes to (CNI)[https://github.com/rit-k8s-rdma/rit-k8s-rdma-sriov-cni] required)\n- Dummy Device Plugin - the current solution requires the use of Device Plugin to give access to  /dev/infiniband  for open issues in Kubernets that can be found  here  and  here . The main problem is Kubernetes does not have the ideas of a  device  directory that  Docker has  --device .",
            "title": "Limitations"
        },
        {
            "location": "/introduction/#future_work",
            "text": "More Vendors - making the solution more interface driven so it can be adapted to more vendors then just Mellanox.  Migrating CNI - the CNI is currently at an older version and we had to bootstrap the newer one, it should be upgraded.  Scheduling - opening up the scheduler to be more adaptable to customizable scheduling algorithms.",
            "title": "Future Work"
        },
        {
            "location": "/introduction/#our_approach_in_short",
            "text": "Problems Addressed:\n - VF's are specified per container\n - Network plugin only ever gives one vf per pod, this means that the device plugin which tracks the amount of available VF's does not maintain an accurate count of VF's being used\n - The VF's selected by Kuberentes to be allocated, may not match those actually allocated by the network plugin  Solution:\n - Device Plugin (RDMA)\n   - Changes to a DameonSet \n   - Stores the current state of the nodes VF resources\n   - Can be quierried through an API\n - Schedular extender\n   - Queries each DameonSet on each node and then filters the possible nodes that a pod can be deployed on based on resource requirements in the annotations for the pod\n - Network Plugin (SRIOV)\n   - Modify it to handle read pods meta-data\n     - Read the amount of VF's\n     - Read the bandwidth limitation on each VF\n   - Modify plugin to set the bandwidth limits from read metadata\n   - Add ability to set  min_tx_rate  and  max_tx_rate",
            "title": "Our Approach in Short"
        }
    ]
}